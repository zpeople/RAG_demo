{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import hashlib\n",
    "USER =\"neo4j\"\n",
    "PWD =\"neo4j123\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92dad0",
   "metadata": {},
   "source": [
    "#### 安装java\n",
    "sudo apt update\n",
    "sudo apt install openjdk-21-jdk\n",
    "\n",
    "\n",
    "#### 解压\n",
    "tar -xzf neo4j-community-2025.05.0-unix.tar.gz\n",
    "cd neo4j-community-2025.05.0\n",
    "\n",
    "#### 设置初始密码\n",
    "./bin/neo4j-admin set-initial-password your_password\n",
    "\n",
    "#### 启动服务\n",
    "./bin/neo4j start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b95998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j\n",
    "neo4j.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(USER, PWD))\n",
    "def test_connection():\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 'Neo4j connection successful!' AS message\")\n",
    "            print(result.single()[\"message\"])\n",
    "    except Exception as e:\n",
    "        print(\"连接失败：\", e)\n",
    "    finally:\n",
    "        driver.close()\n",
    "    \n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Neo4jGraphRAG:\n",
    "    def __init__(self, uri: str, user: str, password: str, \n",
    "                 model_name: str = os.path.join(project_dir, \"model/BAAI/bge-small-zh\")):\n",
    "        \"\"\"\n",
    "        初始化基于Neo4j的Graph RAG系统\n",
    "        \n",
    "        Args:\n",
    "            uri: Neo4j数据库连接URI\n",
    "            user: 数据库用户名\n",
    "            password: 数据库密码\n",
    "            model_name: 用于生成文本嵌入的模型名称\n",
    "        \"\"\"\n",
    "        # 初始化Neo4j连接\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "        # 初始化嵌入模型\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # 确保嵌入向量的索引存在\n",
    "        self._create_vector_index()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"关闭数据库连接\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def _create_vector_index(self):\n",
    "        \"\"\"创建向量索引以加速相似性搜索\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # 检查索引是否已存在\n",
    "            result = session.run(\"\"\"\n",
    "                SHOW INDEXES WHERE name = 'node_embeddings_index'\n",
    "            \"\"\")\n",
    "            \n",
    "            # if  list(result):\n",
    "            #     result = session.run(\"\"\"\n",
    "            #         DROP INDEX `node_embeddings_index`\n",
    "            # \"\"\")\n",
    "           \n",
    "            if not list(result):\n",
    "                # 创建向量索引 \n",
    "                session.run(\"\"\"\n",
    "                        CREATE VECTOR INDEX `node_embeddings_index`\n",
    "                        FOR (n:Entity) ON (n.embedding)\n",
    "                        OPTIONS {\n",
    "                        indexConfig: {\n",
    "                            `vector.dimensions`: 512,\n",
    "                            `vector.similarity_function`: 'cosine'\n",
    "                        }\n",
    "                        }\n",
    "                    \"\"\")\n",
    "\n",
    "    def _normalize_entity_id(self, entity_text: str) -> str:\n",
    "        \"\"\"将字符串转换为固定长度的哈希值\"\"\"\n",
    "        processed = entity_text.lower().strip()\n",
    "        # 使用SHA-256哈希算法，也可以换成md5、sha1等\n",
    "        hash_object = hashlib.sha256(processed.encode('utf-8'))\n",
    "        # 转为16进制字符串（32个字符长度）\n",
    "        return hash_object.hexdigest()\n",
    "    \n",
    "    def add_entity(self, entity_id: str, entity_type: str, properties: Dict, embedding: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        向知识图谱添加实体\n",
    "        \n",
    "        Args:\n",
    "            entity_id: 实体唯一标识符\n",
    "            entity_type: 实体类型（标签）\n",
    "            properties: 实体属性字典\n",
    "            embedding: 实体的预计算嵌入（可选）\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # 如果没有提供嵌入，则从实体描述生成\n",
    "            if embedding is None and \"description\" in properties:\n",
    "                embedding = self.embedding_model.encode(properties[\"description\"])\n",
    "\n",
    "\n",
    "            # 将嵌入向量转换为列表以便存储\n",
    "            embedding_list = embedding.tolist() if embedding is not None else None\n",
    "            if entity_type==\"\"or entity_type==None: \n",
    "                print(entity_type)\n",
    "            # 添加实体节点 \n",
    "            query = f\"\"\"\n",
    "                MERGE (e:{entity_type} {{id: $entity_id}})\n",
    "                SET e += $properties\n",
    "                { 'SET e.embedding = $embedding' if embedding_list else '' }\n",
    "            \"\"\"\n",
    "            session.run(query, {\n",
    "                \"entity_id\": entity_id,\n",
    "                \"properties\": properties,\n",
    "                \"embedding\": embedding_list\n",
    "            })\n",
    "\n",
    "    \n",
    "    def add_relationship(self, source_id: str, target_id: str, relationship_type: str, properties: Dict = None):\n",
    "        \"\"\"\n",
    "        向知识图谱添加实体间的关系\n",
    "        \n",
    "        Args:\n",
    "            source_id: 源实体ID\n",
    "            target_id: 目标实体ID\n",
    "            relationship_type: 关系类型\n",
    "            properties: 关系属性（可选）\n",
    "        \"\"\"\n",
    "        if properties is None:\n",
    "            properties = {}\n",
    "            \n",
    "        with self.driver.session() as session:  \n",
    "            # result= session.run(\"\"\"\n",
    "            #     MATCH (s) WHERE s.id = $source_id\n",
    "            #     MATCH (t) WHERE t.id = $target_id\n",
    "            #     MERGE (s)-[r:%s]->(t)\n",
    "            #     SET r += $properties\n",
    "            # \"\"\" % relationship_type, {\n",
    "            #     \"source_id\": source_id,\n",
    "            #     \"target_id\": target_id,\n",
    "            #     \"properties\": properties\n",
    "            # })\n",
    "            # print(result)\n",
    "            cypher = f\"\"\"\n",
    "            MERGE (s {{id: $source_id}})  \n",
    "            MERGE (t {{id: $target_id}})\n",
    "            MERGE (s)-[r:{relationship_type}]->(t)\n",
    "            SET r += $properties\n",
    "            \"\"\"\n",
    "            session.run(cypher, {\n",
    "                \"source_id\": source_id,\n",
    "                \"target_id\": target_id,\n",
    "                \"properties\": properties\n",
    "            })\n",
    "        \n",
    "    def add_text_chunk(self, chunk_name: str, text: str, related_entities: List[str] = None):\n",
    "        \"\"\"\n",
    "        添加文本块并与相关实体关联\n",
    "        \n",
    "        Args:\n",
    "            chunk_name: 文本块唯一标识符\n",
    "            text: 文本内容\n",
    "            related_entities: 相关实体列表\n",
    "        \"\"\"\n",
    "        # 生成文本嵌入\n",
    "        embedding = self.embedding_model.encode(text)\n",
    "        chunk_id = self._normalize_entity_id(chunk_name)\n",
    "        # print(\"chunk_id \",chunk_id)\n",
    "        with self.driver.session() as session:\n",
    "            # 添加文本块节点并确保其有Entity标签，以便被向量索引包含\n",
    "            session.run(\"\"\"\n",
    "                MERGE (c:TextChunk:Entity {id: $chunk_id})\n",
    "                SET c.name = $name,\n",
    "                    c.content = $text,\n",
    "                    c.embedding = $embedding\n",
    "            \"\"\", {\n",
    "                \"name\":chunk_name,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": text,\n",
    "                \"embedding\": embedding.tolist()\n",
    "            })\n",
    "            \n",
    "            # 与相关实体建立连接\n",
    "            if related_entities:\n",
    "                for entity in related_entities:\n",
    "                    ent_id =self._normalize_entity_id(entity)\n",
    "                    # print(ent_id)\n",
    "                    self.add_relationship(\n",
    "                        chunk_id, \n",
    "                        ent_id, \n",
    "                        \"MENTIONS\",\n",
    "                        {\"s\":chunk_name,\"t\":entity}\n",
    "                    )\n",
    "    \n",
    "    def retrieve_relevant_nodes(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        根据查询检索相关节点\n",
    "        \n",
    "        Args:\n",
    "            query: 查询文本\n",
    "            top_k: 返回的相关节点数量\n",
    "            \n",
    "        Returns:\n",
    "            相关节点信息的列表\n",
    "        \"\"\"\n",
    "        # 生成查询嵌入\n",
    "        query_embedding = self.embedding_model.encode(query)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                CALL db.index.vector.queryNodes('node_embeddings_index', $top_k, $query_embedding)\n",
    "                YIELD node, score\n",
    "                RETURN node.id AS id, labels(node) AS labels, properties(node) AS properties, score\n",
    "            \"\"\", {\n",
    "                \"top_k\": top_k,\n",
    "                \"query_embedding\": query_embedding.tolist()\n",
    "            })\n",
    "            \n",
    "            # print(\"生成查询嵌入\")\n",
    "            return [record.data() for record in result]\n",
    "    \n",
    "    def get_connected_nodes(self, node_id: str, depth: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        获取与指定节点连接的节点\n",
    "        \n",
    "        Args:\n",
    "            node_id: 起始节点ID\n",
    "            depth: 探索深度\n",
    "            \n",
    "        Returns:\n",
    "            连接的节点信息列表\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # 构建Cypher查询，根据深度获取连接节点\n",
    "            match_pattern = \"\"\n",
    "            for i in range(1, depth + 1):\n",
    "                match_pattern += f\"-[r{i}]->(n{i})\"\n",
    "            \n",
    "            nodes_list = [f\"n{i}\" for i in range(1, depth + 1)]\n",
    "            nodes_str = \", \".join(nodes_list)\n",
    "\n",
    "            query = f\"\"\"\n",
    "                MATCH (n0) WHERE n0.id = $node_id\n",
    "                MATCH (n0){match_pattern}\n",
    "                UNWIND [{nodes_str}] AS connected_node\n",
    "                WITH DISTINCT connected_node\n",
    "                RETURN connected_node.id AS id, labels(connected_node) AS labels, properties(connected_node) AS properties\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query, {\"node_id\": node_id})\n",
    "            return [record.data() for record in result]\n",
    "   \n",
    "        \n",
    "    def build_context(self, query: str, top_k: int = 5, context_depth: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        构建回答查询的上下文\n",
    "        \n",
    "        Args:\n",
    "            query: 查询文本\n",
    "            top_k: 检索的相关节点数量\n",
    "            context_depth: 上下文扩展深度\n",
    "            \n",
    "        Returns:\n",
    "            构建的上下文文本\n",
    "        \"\"\"\n",
    "        # 检索相关节点\n",
    "        relevant_nodes = self.retrieve_relevant_nodes(query, top_k)\n",
    "        # print(\"relevant_nodes\",relevant_nodes)\n",
    "        # 收集所有相关节点及其连接节点\n",
    "        context_nodes = {}\n",
    "        \n",
    "        # 添加相关节点\n",
    "        for node in relevant_nodes:\n",
    "            print(\"context_nodes\",node[\"id\"])\n",
    "            context_nodes[node[\"id\"]] = node\n",
    "        \n",
    "        # 添加连接节点\n",
    "        for node in relevant_nodes:\n",
    "            connected_nodes = self.get_connected_nodes(node[\"id\"], context_depth)\n",
    "            # print(connected_nodes)\n",
    "            for conn_node in connected_nodes:\n",
    "                # print(\"conn_node\",conn_node)\n",
    "                if conn_node[\"id\"] not in context_nodes:\n",
    "                    context_nodes[conn_node[\"id\"]] = conn_node\n",
    "                    \n",
    "        \n",
    "        # 构建上下文文本\n",
    "        context_parts = []\n",
    "        \n",
    "        for node_id, node_data in context_nodes.items():\n",
    "            labels = [label for label in node_data[\"labels\"] if label != \"Entity\"]  # 排除基础标签\n",
    "            node_type = labels[0] if labels else \"Node\"\n",
    "            \n",
    "            if node_type == \"TextChunk\":\n",
    "                context_parts.append(f\"Find Text Chunk: {node_data['properties'].get('content', '')}\")\n",
    "            else:\n",
    "                entity_info = [f\"Find Entity:{node_type}: {node_data['properties']['name']}\"]\n",
    "                for key, value in node_data[\"properties\"].items():\n",
    "                    if key not in [\"id\", \"embedding\"]:  # 排除不需要的属性\n",
    "                        entity_info.append(f\"  {key}: {value}\")\n",
    "                \n",
    "                # 添加关系信息\n",
    "                with self.driver.session() as session:\n",
    "                    rel_result = session.run(\"\"\"\n",
    "                        MATCH (n)-[r]->(m) WHERE n.id = $node_id\n",
    "                        RETURN type(r) AS rel_type, m.id AS target_id, labels(m) AS target_labels , properties(r) AS r_properties\n",
    "                    \"\"\", {\"node_id\": node_id})\n",
    "                    \n",
    "                    for rel in rel_result:\n",
    "                        # print(\"----------\",rel['r_properties'])\n",
    "                        target_type = rel[\"target_labels\"][0] if rel[\"target_labels\"] else \"节点\"\n",
    "                        entity_info.append(f\"{rel['r_properties']['s']}  与 {target_type} -{rel['r_properties']['t']} 存在 {rel['rel_type']} 关系\")\n",
    "                \n",
    "                context_parts.append(\"\\n\".join(entity_info))\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_response(self, query: str, top_k: int = 5, context_depth: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        生成基于图谱上下文的回答\n",
    "        \n",
    "        Args:\n",
    "            query: 查询文本\n",
    "            top_k: 检索的相关节点数量\n",
    "            context_depth: 上下文扩展深度\n",
    "            \n",
    "        Returns:\n",
    "            生成的回答\n",
    "        \"\"\"\n",
    "        # 构建上下文\n",
    "        context = self.build_context(query, top_k, context_depth)\n",
    "        \n",
    "        # 构建提示词（实际应用中应替换为真实LLM调用）\n",
    "        prompt = f\"\"\"基于以下上下文信息回答问题:\n",
    "        \n",
    "        上下文:{context}\n",
    "        \n",
    "        问题: {query}\n",
    "        \n",
    "        回答:\"\"\"\n",
    "        \n",
    "        # 模拟LLM输出\n",
    "        simulated_response = f\"关于'{query}'的回答如下：\\n\"\n",
    "        simulated_response += \"在实际应用中，这里会是大型语言模型生成的详细回答。\"\n",
    "        \n",
    "        return simulated_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693536e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 初始化Neo4j Graph RAG系统\n",
    "def manual_Neo4j():\n",
    "    neo4j_rag = Neo4jGraphRAG(\n",
    "        uri=\"bolt://localhost:7687\",\n",
    "        user=USER,\n",
    "        password=PWD\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with neo4j_rag.driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        \n",
    "        # 添加一些实体\n",
    "        neo4j_rag.add_entity(\n",
    "            \"einstein\", \n",
    "            \"Scientist\", \n",
    "            {\"name\": \"阿尔伯特·爱因斯坦\", \"birth_year\": 1879, \n",
    "            \"description\": \"著名物理学家，提出了相对论\"}\n",
    "        )\n",
    "        \n",
    "        neo4j_rag.add_entity(\n",
    "            \"relativity\", \n",
    "            \"Theory\", \n",
    "            {\"name\": \"相对论\", \"description\": \"关于时空和引力的物理理论\"}\n",
    "        )\n",
    "        \n",
    "        neo4j_rag.add_entity(\n",
    "            \"newton\", \n",
    "            \"Scientist\", \n",
    "            {\"name\": \"艾萨克·牛顿\", \"birth_year\": 1643,\n",
    "            \"description\": \"物理学家，提出了万有引力定律和三大运动定律\"}\n",
    "        )\n",
    "        \n",
    "        # 添加实体间的关系\n",
    "        neo4j_rag.add_relationship(\n",
    "            \"einstein\", \n",
    "            \"relativity\", \n",
    "            \"DEVELOPED\",\n",
    "            {\"year\": 1905}\n",
    "        )\n",
    "        \n",
    "        neo4j_rag.add_relationship(\n",
    "            \"einstein\", \n",
    "            \"newton\", \n",
    "            \"WAS_INFLUENCED_BY\"\n",
    "        )\n",
    "        \n",
    "        # 添加文本块\n",
    "        neo4j_rag.add_text_chunk(\n",
    "            \"chunk1\", \n",
    "            \"爱因斯坦在1905年发表了狭义相对论，后来在1915年提出了广义相对论。\",\n",
    "            [\"einstein\", \"relativity\"]\n",
    "        )\n",
    "        \n",
    "        neo4j_rag.add_text_chunk(\n",
    "            \"chunk2\", \n",
    "            \"牛顿的力学理论在低速宏观情况下非常有效，但在高速或强引力场中需要相对论来解释。\",\n",
    "            [\"newton\", \"relativity\"]\n",
    "        )\n",
    "        \n",
    "        # 测试查询\n",
    "        query = \"相对论是谁提出的？它与牛顿的理论有什么关系？\"\n",
    "        print(f\"查询: {query}\")\n",
    "        \n",
    "        # 构建的上下文（用于演示）\n",
    "        print(\"\\n构建的上下文:\")\n",
    "        print(neo4j_rag.build_context(query))\n",
    "\n",
    "        # 获取回答\n",
    "        response = neo4j_rag.generate_response(query, top_k=3, context_depth=2)\n",
    "        # print(\"\\n回答:\")\n",
    "        # print(response)\n",
    "        \n",
    "        \n",
    "    finally:\n",
    "        # 关闭连接\n",
    "        neo4j_rag.close()\n",
    "\n",
    "# manual_Neo4j()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a90721",
   "metadata": {},
   "source": [
    "## Auto Graph RAG\n",
    "\n",
    "### 三种匹配方式\n",
    "\n",
    "**re（正则表达式）**\n",
    "re 的核心优势在于高效精准与低门槛：基于明确字符规则匹配，处理结构化文本（如固定格式的手机号、日期）时速度极快，且无需依赖复杂模型，仅需编写规则即可快速落地，成本极低。但它的局限性也很明显，缺乏语义理解且适配性弱—— 只能识别字符形式，无法区分 “苹果（水果）” 和 “苹果（品牌）” 这类语义差异，面对非结构化文本（如自由对话）完全失效，且文本格式稍有变化就需重新调整规则，维护成本高。\n",
    "\n",
    "**spaCy（NLP 工具库）**\n",
    "spaCy 的核心价值是平衡语义能力与实用效率：作为预训练 NLP 工具，它自带实体识别（NER）、词性标注等功能，能理解文本语义（如自动识别 “北京” 是地名、“张三” 是人名），且无需从零训练模型，开箱即用，推理速度比 LLM 快得多，适合中等规模的结构化 NLP 任务。不过它的泛化性和灵活性不足—— 对医疗、法律等小众领域的专业术语识别准确率低，需额外微调才能适配，且无法处理 “提取用户抱怨的问题” 这类模糊需求，只能应对明确的实体或语法层面的匹配任务。\n",
    "\n",
    "**LLM（大语言模型）**\n",
    "LLM 的核心优势是超强语义理解与泛化能力：依托大规模语料训练，它能处理复杂、模糊的需求（如 “提取用户提到的、需要优先解决的产品故障”），不仅能识别多义、小众术语，还能结合上下文做逻辑判断，甚至排除无效信息。同时，它无需针对特定领域微调，就能适配多数场景，对非结构化文本的处理能力远超 re 和 spaCy。但它的短板也很突出—— 推理依赖大模型，处理大量文本时速度慢、算力 / API 成本高，且可能出现 “幻觉”（虚构匹配结果），结果可控性差，还会受输入文本质量影响，上下文不清晰时准确率大幅下降。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fac3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "spacy.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用spaCy的中等规模模型\n",
    "# 加载NLP模型用于实体识别和关系抽取\n",
    "spacy_model =\"zh_core_web_sm\"\n",
    "try:\n",
    "    # 使用spaCy的中等规模模型\n",
    "    nlp = spacy.load(spacy_model)\n",
    "except:\n",
    "    # 如果没有安装，自动下载\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(spacy_model)\n",
    "    nlp = spacy.load(spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4483c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "MODEL_NAME = \"qwen-plus\"\n",
    "api_key =  os.getenv(\"QWEN_API_KEY\") or os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "def build_prompt(text):\n",
    "    \"\"\"\n",
    "    构建结构化提示词：告诉大模型需要做什么、输出格式是什么\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    请处理以下中文文本，完成实体与关系提取任务：\n",
    "\n",
    "    # 任务目标\n",
    "    从文本中提取两类关键信息：\n",
    "    1）**实体**：现实世界中存在或概念化的具体/抽象事物，包括但不限于人物、组织、地点、事件、概念、产品、时间、机构等\n",
    "    2）**实体间的关系**：描述实体间的语义关联，如\"任职于\"、\"位于\"、\"发布于\"、\"属于\"等\n",
    "\n",
    "    # 提取规则\n",
    "    ## 实体提取：\n",
    "    - 保留完整名称（如\"阿里巴巴集团\"而非\"阿里巴巴\"，\"2024年世界人工智能大会\"而非\"AI大会\"）\n",
    "    - 同一实体的不同表述需合并（如\"苹果公司\"与\"Apple Inc.\"统一标注为\"苹果公司（Apple Inc.）\"）\n",
    "    - 过滤无意义的虚词、副词（如\"的\"、\"非常\"、\"可能\"），仅保留具有实际指代意义的名词/名词短语\n",
    "    - 实体类型建议：人物、组织、公司、机构、地点、城市、国家、事件、产品、技术、概念、时间、日期、指标等\n",
    "\n",
    "    ## 关系提取：\n",
    "    - 关系需明确、无歧义（如\"小明在腾讯工作\"应提取为\"小明-任职于-腾讯\"，而非\"小明-在-腾讯\"）\n",
    "    - 若文本中未直接说明但可逻辑推导明确的关系，需补充完整（如\"张三是华为的CEO\"→\"张三-担任CEO-华为\"）\n",
    "    - 排除冗余关系（同一实体对的相同关系仅保留1次）\n",
    "    - 关系必须基于文本内容，不可虚构\n",
    "    - 代词需替换为对应的实体（如\"他提出了狭义相对论\"→主语为\"阿尔伯特·爱因斯坦\"）\n",
    "    \n",
    "    # 输出格式（严格遵循）\n",
    "    - 仅输出纯 JSON 内容，不要包含任何多余文本\n",
    "    - 禁止使用任何代码块标记（如 ```json、``` 等）\n",
    "    - 确保 JSON 格式正确，可被直接解析\n",
    "    - id = re.sub(r'\\W+', '_', name.lower().strip())\n",
    "    {{\n",
    "    \"entities\": [\n",
    "        {{\n",
    "        \"name\": \"示例实体1\",\n",
    "        \"label\": \"人物\" \n",
    "        }},\n",
    "        {{\n",
    "        \"name\": \"示例实体2\",\n",
    "        \"label\": \"组织\"\n",
    "        }}\n",
    "    ],\n",
    "    \"relations\": [\n",
    "        {{\n",
    "        \"source\": \"示例实体1\",\n",
    "        \"type\": \"任职于\",\n",
    "        \"target\": \"示例实体2\"\n",
    "        }}\n",
    "    ]\n",
    "    }}\n",
    "\n",
    "    文本内容：\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def add_id_to_single_item(item):\n",
    "    \"\"\"新增ID（核心处理逻辑）\"\"\"\n",
    "    def get_string_hash(s: str) -> str:\n",
    "        \"\"\"将字符串转换为固定长度的哈希值\"\"\"\n",
    "        processed = s.lower().strip()\n",
    "        # 使用SHA-256哈希算法，也可以换成md5、sha1等\n",
    "        hash_object = hashlib.sha256(processed.encode('utf-8'))\n",
    "        # 转为16进制字符串（32个字符长度）\n",
    "        return hash_object.hexdigest()\n",
    "    \n",
    "    if isinstance(item, dict):\n",
    "        # 若元素是字典：复制后添加ID（避免覆盖原字典）\n",
    "        new_item = item.copy()\n",
    "        if \"id\" not in new_item:  # 已有ID则不重复添加\n",
    "            new_item[\"id\"] =get_string_hash(item['name'])\n",
    "        return new_item\n",
    "\n",
    "\n",
    "        \n",
    "def extract_entities_relations_llm(text):\n",
    "    \"\"\"\n",
    "    调用大模型提取关系，并解析 JSON 结果\n",
    "    \"\"\"\n",
    "\n",
    "    # 构建提示词\n",
    "    prompt = build_prompt(text)\n",
    "    \n",
    "    client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=BASE_URL,\n",
    "            )\n",
    "\n",
    "    output = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{prompt}\"}],\n",
    "            model=MODEL_NAME,\n",
    "            stream=False,\n",
    "        )\n",
    "    output=output.choices[0].message.content\n",
    "    # print(output)\n",
    "    # 提取输出中的 JSON 部分（大模型可能会多输出一些文字，需过滤）\n",
    "    import re\n",
    "    json_str = re.sub(r'^```json\\s*|\\s*```$', '', output.strip(), flags=re.MULTILINE)\n",
    "\n",
    "    #  解析 JSON 为结构化列表\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "\n",
    "        # 拆分出entities和relations\n",
    "        entities = data.get('entities', [])\n",
    "        relations = data.get('relations', [])\n",
    "        return entities,relations\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"大模型输出的 JSON 格式错误\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f794c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoGraphRAG(Neo4jGraphRAG):\n",
    "    def __init__(self, uri, user, password, model_name = os.path.join(project_dir, \"model/BAAI/bge-small-zh\"),spacy_model =\"zh_core_web_sm\"):\n",
    "        super().__init__(uri, user, password, model_name)\n",
    "        self.nlp  = spacy.load(spacy_model)\n",
    "        # 关系抽取的模式（可根据需求扩展）\n",
    "        self.relationship_patterns = [\n",
    "            (r\"(.+) (发明|发现|提出)了? (.+)\", r\"\\2\"),        # 例如：爱因斯坦提出了相对论\n",
    "            (r\"(.+) (影响|启发)了? (.+)\", r\"\\2\"),             # 例如：牛顿影响了爱因斯坦\n",
    "            (r\"(.+) (出生于|生于) (.+)\", r\"\\2\"),              # 例如：爱因斯坦出生于1879年\n",
    "            (r\"(.+) 是 (.+)\", r\"\\2\"),                         # 例如：爱因斯坦是物理学家\n",
    "            (r\"(.+) (发表|撰写)了? (.+)\", r\"\\2\")              # 例如：爱因斯坦发表了论文\n",
    "        ]\n",
    "        \n",
    "    def _extract_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"从文本中提取实体\"\"\"\n",
    "        doc = self.nlp (text)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # 去重并简化实体\n",
    "            if ent.text not in [e[\"name\"] for e in entities]:\n",
    "                entities.append({\n",
    "                    \"name\": ent.text,\n",
    "                    \"label\": ent.label_,  # 实体类型（如PERSON, ORG, GPE等）\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _extract_relationships(self, text: str) -> List[Dict]:\n",
    "        \"\"\"从文本中提取关系\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # 使用规则匹配提取关系\n",
    "        for pattern, rel_type in self.relationship_patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                print(match)\n",
    "                s_obj = match.group(1).strip()\n",
    "                t_obj = match.group(3).strip()\n",
    "                \n",
    "                print(rel_type,s_obj,t_obj)\n",
    "                # 只添加有效的实体关系\n",
    "                if s_obj and t_obj and s_obj != t_obj:\n",
    "                    relationships.append({\n",
    "                        \"source\": s_obj,\n",
    "                        \"target\": t_obj,\n",
    "                        \"type\": rel_type.upper()\n",
    "                    })\n",
    "        \n",
    "        # 使用spaCy的依赖解析辅助提取关系\n",
    "        '''\n",
    "        # 添加共指消解组件  coreferee不支持zh\n",
    "        nlp.add_pipe(\"coreferee\")\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # 构建共指映射：将代词映射到原始实体\n",
    "        coref_map = {}\n",
    "        for cluster in doc._.coreferee.resolve_doc():\n",
    "            # 取聚类中最长的实体作为代表（通常是完整名称）\n",
    "            main_entity = max(cluster, key=lambda x: len(x.text))\n",
    "            for mention in cluster:\n",
    "                if mention.text != main_entity.text:\n",
    "                    coref_map[mention.text] = main_entity.text\n",
    "        \n",
    "        # 替换文本中的代词\n",
    "        resolved_tokens = []\n",
    "        for token in doc:\n",
    "            resolved_tokens.append(coref_map.get(token.text, token.text))\n",
    "        resolved_text = \" \".join(resolved_tokens).replace(\"  \", \" \")  # 清理多余空格\n",
    "        print(\"共指消解后的文本：\")\n",
    "        print(resolved_text)\n",
    "        print(\"\\n提取的关系：\")\n",
    "        \n",
    "        # 重新分析处理后的文本\n",
    "        doc_resolved = nlp(resolved_text)\n",
    "        '''\n",
    "   \n",
    "        doc_resolved = nlp(text)\n",
    "        for token in doc_resolved:\n",
    "        # 寻找主语和宾语，它们通常依附于动词\n",
    "            if token.dep_ in [\"nsubj\", \"dobj\", \"pobj\"] and token.head.pos_ == \"VERB\":\n",
    "                s_obj = None\n",
    "                t_obj = None\n",
    "                \n",
    "                # 查找主语和宾语\n",
    "                for child in token.head.children:\n",
    "                    if child.dep_ == \"nsubj\":\n",
    "                        s_obj = child.text\n",
    "                    if child.dep_ in [\"dobj\", \"pobj\"]:\n",
    "                        t_obj = child.text\n",
    "                if s_obj and t_obj:\n",
    "                    # print(f\"head: {token.head.text}, pos: {token.head.pos_},lemma:{ token.head.lemma_}\") \n",
    "                    # rel_type = token.head.lemma_.upper()\n",
    "                    rel_type =token.head.text # 中文lemma为空\n",
    "               \n",
    "                    # 避免重复添加\n",
    "                    if not any(r[\"source\"] == s_obj and r[\"target\"] == t_obj and r[\"type\"] == rel_type \n",
    "                               for r in relationships):\n",
    "                        relationships.append({\n",
    "                            \"source\": s_obj,\n",
    "                            \"target\": t_obj,\n",
    "                            \"type\": rel_type\n",
    "                        })\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "\n",
    "    \n",
    "    def add_document(self, doc_id: str, text: str,isLLm=False):\n",
    "        \"\"\"添加文档并自动抽取实体和关系\"\"\"\n",
    "        # 提取实体和关系\n",
    "        if isLLm:\n",
    "            entities,relationships = extract_entities_relations_llm(text)\n",
    "        else:\n",
    "            entities = self._extract_entities(text)\n",
    "            relationships = self._extract_relationships(text)\n",
    "  \n",
    "       \n",
    "        \n",
    "        # 添加实体\n",
    "        for ent in entities:\n",
    "            # print(ent[\"name\"])\n",
    "            ent_id =self._normalize_entity_id(ent[\"name\"])\n",
    "            # print(\"ent_id\",ent_id)\n",
    "            self.add_entity(\n",
    "                ent_id, \n",
    "                ent[\"label\"],  # 实体类型作为标签\n",
    "                {\"name\": ent[\"name\"], \"description\": f\"从文本中提取的{ent['label']}实体\"}\n",
    "            )\n",
    "        \n",
    "        # 添加关系\n",
    "        for rel in relationships:\n",
    "            source_id = self._normalize_entity_id(rel[\"source\"])\n",
    "            target_id = self._normalize_entity_id(rel[\"target\"])\n",
    "            \n",
    "            if not isLLm:\n",
    "                # spacy解析可能不包含实体，确保主体和客体实体存在\n",
    "                self.add_entity(\n",
    "                    source_id, \n",
    "                    \"Entity\",  # 默认类型，如果不存在\n",
    "                    {\"name\": rel[\"source\"]}\n",
    "                )\n",
    "                self.add_entity(\n",
    "                    target_id, \n",
    "                    \"Entity\", \n",
    "                    {\"name\": rel[\"target\"]}\n",
    "                )\n",
    "            \n",
    "            self.add_relationship(source_id, target_id, rel[\"type\"],{\"s\":rel[\"source\"],\"t\":rel[\"target\"]})\n",
    "      \n",
    "         # 添加文本块(避免重复)\n",
    "        self.add_text_chunk(doc_id, text, [e[\"name\"] for e in entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    auto_rag = AutoGraphRAG(\n",
    "            uri=\"bolt://localhost:7687\",\n",
    "            user=USER,\n",
    "            password=PWD\n",
    "        )\n",
    "    driver = auto_rag.driver\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        \n",
    "            # 自动处理文档并构建知识图谱\n",
    "            doc1 = \"\"\"爱因斯坦是一位著名的物理学家，1879年出生于德国。\n",
    "            爱因斯坦在1905年提出了狭义相对论，1915年发表了广义相对论。\n",
    "            爱因斯坦受到了牛顿的影响，但他的理论扩展了牛顿力学的适用范围。\"\"\"\n",
    "            \n",
    "            doc2 = \"\"\"牛顿是17世纪的英国物理学家，他提出了万有引力定律和三大运动定律。\n",
    "            这些理论为经典力学奠定了基础，影响了后来包括爱因斯坦在内的许多科学家。\"\"\"\n",
    "            \n",
    "            #对比 spacy和llm 的提取\n",
    "            entities =auto_rag._extract_entities(doc1)\n",
    "            print(\"e \",entities)\n",
    "            relations =auto_rag._extract_relationships(doc1)\n",
    "            print(\"r \",relations)\n",
    "\n",
    "            entities,relations = extract_entities_relations_llm(doc1)\n",
    "            print(\"e \",entities)\n",
    "            print(\"r \",relations)\n",
    "\n",
    "            # 添加文档（自动抽取实体和关系）\n",
    "            auto_rag.add_document(\"doc1\", doc1,True)\n",
    "            auto_rag.add_document(\"doc2\", doc2,True)\n",
    "            #裸节点\n",
    "            session.run(\n",
    "                '''\n",
    "                MATCH (n)\n",
    "                WHERE size(labels(n)) = 0 AND n.id IS NOT NULL\n",
    "                DETACH DELETE n\n",
    "                '''\n",
    "            )\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "# create_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查节点和关系\n",
    "auto_rag = AutoGraphRAG(\n",
    "        uri=\"bolt://localhost:7687\",\n",
    "        user=USER,\n",
    "        password=PWD\n",
    "    )\n",
    "driver = auto_rag.driver\n",
    "with driver.session() as session:\n",
    "\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.id IS NOT NULL\n",
    "        WITH n.id AS id, collect(n) AS nodes, count(*) AS cnt\n",
    "        WHERE cnt > 1\n",
    "        UNWIND nodes AS node\n",
    "        RETURN id,cnt, labels(node) AS labels, properties(node) AS props\n",
    "    \"\"\")\n",
    "    print([(record[\"id\"],record['labels']) for record in result]) \n",
    "\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN n.id AS id, labels(n) AS labels, properties(n) AS props\n",
    "    \"\"\")\n",
    "    print([(record[\"id\"],record['labels']) for record in result]) \n",
    "    \n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN DISTINCT labels(n) AS labels\n",
    "    \"\"\")\n",
    "    print([record[\"labels\"] for record in result]) \n",
    "\n",
    "\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH ()-[r]->()\n",
    "        RETURN DISTINCT type(r) AS relation_type\n",
    "    \"\"\")\n",
    "    print([record[\"relation_type\"] for record in result]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82610c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试相关节点查询\n",
    "auto_rag = AutoGraphRAG(\n",
    "        uri=\"bolt://localhost:7687\",\n",
    "        user=USER,\n",
    "        password=PWD\n",
    "    )\n",
    "    \n",
    "driver = auto_rag.driver\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "       # 测试查询\n",
    "        query = \"爱因斯坦提出了什么理论？他受到了谁的影响？\"\n",
    "        print(f\"查询: {query}\")\n",
    "      \n",
    "      # 查看构建的上下文\n",
    "        print(\"\\n构建的上下文:\")\n",
    "        print(auto_rag.build_context(query))\n",
    "        \n",
    "          # 获取回答\n",
    "        # response = auto_rag.generate_response(query)\n",
    "        # print(\"\\n回答:\")\n",
    "        # print(response)\n",
    "finally:\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec80201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from openai import OpenAI\n",
    "from pydantic import Field\n",
    "\n",
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "ONLINE_MODEL_NAME = \"qwen-plus\"\n",
    "\n",
    "\n",
    "class OpenAILLM(LLM):\n",
    "    \"\"\"\n",
    "    一个将OpenAI API包装为LangChain LLM的自定义类\n",
    "    \"\"\"\n",
    "\n",
    "    api_key: Optional[str] = Field(None)\n",
    "    base_url: Optional[str] = Field(BASE_URL)\n",
    "    model_name: str = Field(ONLINE_MODEL_NAME)\n",
    "    temperature: float = Field(0.7)\n",
    "    max_tokens: int = Field(1024)\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        # 配置OpenAI API密钥\n",
    "        if self.api_key:\n",
    "            OpenAI.api_key = self.api_key\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"返回LLM类型标识\"\"\"\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"返回用于标识LLM的参数\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "        }\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        实现调用OpenAI API的核心方法\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 调用OpenAI的 completions API\n",
    "            client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=self.base_url,\n",
    "            )\n",
    "            completion = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "            # 提取并返回生成的文本\n",
    "            return completion.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"调用OpenAI API时发生错误: {str(e)}\")\n",
    "\n",
    "    def predict(self, text: str, **kwargs: Any) -> str:\n",
    "        \"\"\"预测方法，与LangChain其他组件兼容\"\"\"\n",
    "        return self._call(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351acdd",
   "metadata": {},
   "source": [
    "https://github.com/neo4j/apoc/releases/tag/2025.05.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=USER,\n",
    "    password=PWD,\n",
    "    refresh_schema=True\n",
    ")\n",
    "print(graph.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "215e0e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:人物 {name: \"爱因斯坦\"})-[:提出]->(t:理论)\n",
      "WITH p, collect(t.name) AS 提出的理论\n",
      "MATCH (p)-[:受到影响于]->(influencer:人物)\n",
      "RETURN \n",
      "  提出的理论 AS 爱因斯坦提出的理论,\n",
      "  collect(influencer.name) AS 受到影响的人物\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'爱因斯坦提出的理论': ['狭义相对论'], '受到影响的人物': ['牛顿']}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "api_key =  os.getenv(\"QWEN_API_KEY\") or os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "ONLINE_MODEL_NAME = \"qwen-plus\"\n",
    "prompt=\"爱因斯坦提出了什么理论？他受到了谁的影响？\"\n",
    "llm = OpenAILLM(model_name=ONLINE_MODEL_NAME, base_url=BASE_URL, api_key=api_key)\n",
    "\n",
    "cypher_template = \"\"\"\n",
    "你需要根据以下中文图结构生成Cypher查询，解决用户问题：\n",
    "\n",
    "图结构：\n",
    "{schema}\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "生成规则：\n",
    "1. 必须使用中文标签（如:人物）、中文关系（如-[提出]->）、中文属性（如姓名）\n",
    "2. 不要使用英文标签/关系/属性\n",
    "3. 确保查询语法正确\n",
    "\n",
    "生成的Cypher：\n",
    "\"\"\"\n",
    "cypher_prompt = PromptTemplate(\n",
    "    template=cypher_template,\n",
    "    input_variables=[graph.schema, \"question\"]\n",
    ")\n",
    "qa_prompt = PromptTemplate.from_template(\"根据查询结果回答问题：\\n{context}\\n问题：{question}\")\n",
    "\n",
    "cypher_generation_chain = cypher_prompt | llm\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "# 构建 GraphQAChain\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    # graph_schema=graph.schema,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt =qa_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "# 执行问答\n",
    "response = chain.run({\"query\": prompt})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
