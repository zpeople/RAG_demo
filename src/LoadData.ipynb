{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d6077a",
   "metadata": {},
   "source": [
    "加载文件：使用langchain下的document_loaders加载pdf、docs、txt、md等格式文件\n",
    "\n",
    "文本分块：分块的方式有很多，选择不同的分块方法、分块大小、chunk_overlap，对最后的检索结果有影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cf4e5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from tool import skip_execution\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader,\n",
    ")\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    RecursiveJsonSplitter,\n",
    ")\n",
    "import json\n",
    "\n",
    "IS_SKIP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a55aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    \"\"\"\n",
    "    加载PDF、DOC、TXT文档\n",
    "    \"\"\"\n",
    "    name, extension = os.path.splitext(file)\n",
    "    if extension == \".pdf\":\n",
    "        print(f\"Loading {file}\")\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == \".docx\":\n",
    "        print(f\"Loading {file}\")\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == \".txt\":\n",
    "        loader = UnstructuredFileLoader(file)\n",
    "        print(f\"Loading {file}\")\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "def chunk_data(data, chunk_size=256, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    将数据分割成块\n",
    "    :param data:\n",
    "    :param chunk_size: chunk块大小\n",
    "    :param chunk_overlap: 重叠部分大小\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    print(f\"pages: {len(data)}\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b48aa785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_load 已跳过执行\n"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_load():\n",
    "    path = os.path.join(project_dir, \"datasets\", \"test.pdf\")\n",
    "\n",
    "    print(os.path.abspath(path))\n",
    "    data = load_document(path)\n",
    "    print(\"data pages:\", len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = test_load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40558c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_chunk 已跳过执行\n"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_chunk(data):\n",
    "    chunks = chunk_data(data)\n",
    "    print(\"chunks len:\", len(chunks))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "test_chunk(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bdca4",
   "metadata": {},
   "source": [
    "### Load Json to document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43e143",
   "metadata": {},
   "source": [
    "整理JSON格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5214652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def parse_legal_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    解析原始法律问答数据，转换为适合RAG系统使用的结构化格式\n",
    "\n",
    "    参数:\n",
    "        raw_data: 原始JSON数据\n",
    "\n",
    "    返回:\n",
    "        结构化后的法律问答数据\n",
    "    \"\"\"\n",
    "    # 优先获取input字段\n",
    "    user_query = raw_data.get(\"input\", \"\").strip()\n",
    "\n",
    "    # 如果input为空，则使用instruction字段\n",
    "    if not user_query:\n",
    "        user_query = raw_data.get(\"instruction\", \"\").strip()\n",
    "\n",
    "    # 移除多余的标点和重复字符\n",
    "    user_query = re.sub(r\"[\\n\\r]+\", \" \", user_query)\n",
    "    user_query = re.sub(r\" +\", \" \", user_query)\n",
    "    user_query = re.sub(r\"，+\", \"，\", user_query)\n",
    "    user_query = re.sub(r\"？+\", \"？\", user_query)\n",
    "\n",
    "    # 处理回答内容，分离回答和法律依据\n",
    "    output = raw_data.get(\"output\", \"\").strip()\n",
    "\n",
    "    # 提取回答部分\n",
    "    answer_match = re.match(r\"回答:(.*?)法律依据:\", output, re.DOTALL)\n",
    "    answer = \"\"\n",
    "    if answer_match:\n",
    "        answer = answer_match.group(1).strip()\n",
    "    else:\n",
    "        # 如果没有明确分隔，尝试提取到第一个法律条文前\n",
    "        # answer = re.split(r'《\\w+法》', output, 1)[0].replace('回答:', '').strip()\n",
    "        answer = output\n",
    "\n",
    "    # 提取法律依据部分\n",
    "    legal_basis_text = re.sub(r\"^回答:.*?法律依据:\", \"\", output, flags=re.DOTALL).strip()\n",
    "    legal_basis = []\n",
    "\n",
    "    # 正则匹配法律条文（如《民事诉讼法》第二百四十三条）\n",
    "    law_pattern = re.compile(\n",
    "        r\"《(.*?)》(第?\\s*[\\d一二三四五六七八九十]+条?)\\s*规定?，?(.*?)(?=《|$)\", re.DOTALL\n",
    "    )\n",
    "    matches = law_pattern.findall(legal_basis_text)\n",
    "\n",
    "    for match in matches:\n",
    "        law_name, article, content = match\n",
    "        # 清理内容\n",
    "        content = content.strip().replace(\"\\n\", \" \")\n",
    "        content = re.sub(r\" +\", \" \", content)\n",
    "\n",
    "        legal_basis.append(\n",
    "            {\n",
    "                \"law_name\": law_name.strip(),\n",
    "                \"article\": article.strip().replace(\"第\", \"\").replace(\"条\", \"\"),\n",
    "                \"content\": content,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 构建结构化数据\n",
    "    structured_data = {\n",
    "        \"user_query\": user_query,\n",
    "        \"answer\": answer,\n",
    "        \"legal_basis\": legal_basis,\n",
    "    }\n",
    "    meta_data = {\"id\": raw_data.get(\"id\", \"\")}\n",
    "\n",
    "    return structured_data, meta_data\n",
    "\n",
    "\n",
    "def batch_process(\n",
    "    raw_data_list: List[Dict[str, Any]]\n",
    ") -> (List[Dict[str, Any]], List[Dict[str, Any]]):\n",
    "    \"\"\"批量处理数据列表，分别返回结构化数据和元数据\"\"\"\n",
    "    structured_list = []\n",
    "    meta_list = []\n",
    "\n",
    "    for data in raw_data_list:\n",
    "        structured, meta = parse_legal_data(data)\n",
    "        structured_list.append(structured)\n",
    "        meta_list.append(meta)\n",
    "\n",
    "    return structured_list, meta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def load_json(file):\n",
    "    \"\"\"\n",
    "    加载JSON\n",
    "    \"\"\"\n",
    "    name, extension = os.path.splitext(file)\n",
    "    if extension == \".json\":\n",
    "        # 处理JSON文件\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "                # 将JSON数据转换为Document 1json-->1 document\n",
    "                # json_content = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "                # data = [Document(\n",
    "                #     page_content=json_content,\n",
    "                #     metadata={'source': file}\n",
    "                # )]\n",
    "\n",
    "                # 按每条instruction 拆分\n",
    "                json_data, meta_data = batch_process(json_data)\n",
    "                datas = [\n",
    "                    Document(page_content=str(chunk), metadata=meta)\n",
    "                    for chunk, meta in zip(json_data, meta_data)\n",
    "                ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JSON file: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Document format is not supported!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"pages: {len(datas)}\")\n",
    "\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_load_json 已跳过执行\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_load_json():\n",
    "    path = os.path.join(project_dir, \"datasets\", \"chinese_law_ft_dataset_mini.json\")\n",
    "    print(os.path.abspath(path))\n",
    "    data = load_json(path)\n",
    "    print(\"data pages:\", len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "data = test_load_json()\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4336537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_chunk 已跳过执行\n"
     ]
    }
   ],
   "source": [
    "test_chunk(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
