import os
import sys
import time
try:
    get_ipython
    current_dir = os.getcwd()
except NameError:
    current_dir = os.path.dirname(os.path.abspath(__file__))

# è®¾ç½®è·¯å¾„ï¼Œä¸´æ—¶æ‰©å±•è·¯å¾„
project_dir = os.path.abspath(os.path.join(current_dir, '..'))
if project_dir not in sys.path:
    sys.path.append(project_dir)

import streamlit as st
import models as M
import Embedding as emb
from LoadData import load_document, chunk_data
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


emb_name = "BAAI/bge-small-zh"
model_name = "Qwen/Qwen3-0.6B"
online_model_name ="qwen-plus"
online_URL= "https://dashscope.aliyuncs.com/compatible-mode/v1"
DEFAULT_TEMPLATE = """
        ä½ æ˜¯ä¸€ä¸ªèªæ˜çš„è¶…çº§æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸“ä¸šä¸”å¯Œæœ‰é€»è¾‘é¡ºåºçš„å¥å­å›å¤ï¼Œå¹¶ä»¥ä¸­æ–‡å½¢å¼ä¸”markdownå½¢å¼è¾“å‡ºã€‚
        æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼š
        {context}
        é—®é¢˜ï¼š
        {question}
    """

# è¿è¡Œåº”ç”¨: streamlit run ./st_app.py

vector_db_path = os.path.join(project_dir, "db", "law_db")
print(f"vector_db_path: {vector_db_path}")


# æ¸…é™¤streamlitä¼šè¯çŠ¶æ€ä¸­çš„èŠå¤©å†å²
def clear_history():
    if 'history' in st.session_state:
        del st.session_state['history']
    if 'messages' in st.session_state:
        del st.session_state['messages']

# æµå¼è¾“å‡ºç”Ÿæˆå™¨
def stream_answer_generator(model_name, vector_db, prompt, top_k):
    """ç”Ÿæˆæµå¼å“åº”çš„ç”Ÿæˆå™¨å‡½æ•°"""
    # è·å–ç›¸å…³æ–‡æ¡£
    docs = vector_db.similarity_search(prompt, k=top_k)
    context = "\n".join([doc.page_content for doc in docs])
    
    # æ„å»ºå®Œæ•´æç¤º
    full_prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}\n\nå›ç­”ï¼š"
    
    # é€æ­¥ç”Ÿæˆå“åº”
    response = ""
    for chunk in M.generate_streaming_response(model_name, full_prompt):
        response += chunk
        yield chunk
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºçš„å»¶è¿Ÿï¼ˆå®é™…ä½¿ç”¨ä¸­å¯å»æ‰ï¼‰
        # time.sleep(0.05)


if __name__ == "__main__":
    # é¡µé¢è®¾ç½®
    st.set_page_config(page_title="LLMé—®ç­”åº”ç”¨", page_icon="ğŸ¤–")
    
    st.subheader('LLM Question-Answering Application ğŸ¤–')
    with st.sidebar:
       
        # é€‰æ‹©æœ¬åœ°å¤§æ¨¡å‹
        llm = st.selectbox(
            label="è¯·é€‰æ‹©å¤§æ¨¡å‹",
            options=(model_name,online_model_name, )
        )
        
        # é€‰æ‹©å‘é‡æ•°æ®åº“
        embedding = st.selectbox(
            "è¯·é€‰æ‹©å‘é‡æ•°æ®åº“",
            ('FAISS', 'Chroma')
        )

        # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
        uploaded_file = st.file_uploader('ä¸Šä¼ æ–‡ä»¶:', type=['pdf', 'docx', 'txt'])

        #  chunkå¤§å°è®¾ç½®
        chunk_size = st.number_input(
            'chunk_size:', 
            min_value=100, 
            max_value=2048, 
            value=512, 
            on_change=clear_history
        )

        # chunké‡å è®¾ç½®
        chunk_overlap = st.number_input(
            label="chunk_overlap", 
            min_value=0, 
            max_value=1024, 
            value=150,
            on_change=clear_history
        )

        # æ£€ç´¢æ•°é‡è®¾ç½®
        k = st.number_input(
            'top_k', 
            min_value=1, 
            max_value=20, 
            value=3, 
            on_change=clear_history
        )

        # æ·»åŠ æ•°æ®æŒ‰é’®
        add_data = st.button('æ·»åŠ æ•°æ®', on_click=clear_history)

       

        if uploaded_file and add_data:  # å¦‚æœç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶
            with st.spinner('æ­£åœ¨è¯»å–ã€åˆ†å‰²å’ŒåµŒå…¥æ–‡ä»¶...'):
                # å°†æ–‡ä»¶ä»RAMå†™å…¥å½“å‰ç›®å½•
                bytes_data = uploaded_file.read()
                file_name = os.path.join('./', uploaded_file.name)
                with open(file_name, 'wb') as f:
                    f.write(bytes_data)

                data = load_document(file_name)
                chunks = chunk_data(data, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                st.write(f'Chunkå¤§å°: {chunk_size}, é‡å åº¦: {chunk_overlap}, æ€»ç‰‡æ®µæ•°: {len(chunks)}')

                # åˆ›å»ºåµŒå…¥å¹¶è¿”å›å‘é‡å­˜å‚¨
                if embedding == "FAISS":
                    vector_store = emb.create_embeddings_faiss(
                        embedding_name=emb_name,
                        chunks=chunks,
                        # vector_db_path=vector_db_path, 
                    )
                elif embedding == "Chroma":
                    vector_store = emb.create_embeddings_chroma(emb_name,chunks)

                # å°†å‘é‡å­˜å‚¨ä¿å­˜åœ¨streamlitä¼šè¯çŠ¶æ€ä¸­
                st.session_state.vs = vector_store
                st.success('æ–‡ä»¶ä¸Šä¼ ã€åˆ†å‰²å’ŒåµŒå…¥æˆåŠŸã€‚')

    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # å±•ç¤ºå¯¹è¯å†å²
    for msg in st.session_state.messages:
        with st.chat_message(msg['role']):
            st.markdown(msg["content"])

    # åŠ è½½é»˜è®¤å‘é‡æ•°æ®åº“ï¼ˆå¦‚æœå°šæœªåŠ è½½ï¼‰
    if 'vs' not in st.session_state:
        try:
            with st.spinner('æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...'):
                vector_store = emb.load_embeddings_faiss(emb_name,vector_db_path)
                st.session_state.vs = vector_store
                st.toast('å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ!', icon='ğŸ˜')
        except Exception as e:
            st.warning(f'åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}', icon="âš ï¸")

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})

        # è·å–å‘é‡å­˜å‚¨
        vector_store = st.session_state.get('vs')
        
        if vector_store is not None:
            # æœ¬åœ°æ¨¡å‹
            if llm == model_name:
                with st.spinner('æ­£åœ¨ç”Ÿæˆå›ç­”...'):
                    response = M.ask_and_get_answer_from_local(
                        model_name=model_name, 
                        vector_db=vector_store, 
                        prompt=prompt, 
                        template=DEFAULT_TEMPLATE,
                        top_k=k
                    )
                    
                    # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
                    with st.chat_message("assistant"):
                        st.markdown(response)
                    # æ·»åŠ åˆ°å¯¹è¯å†å²
                    st.session_state.messages.append({"role": "assistant", "content": response})
            
            # åœ¨çº¿æ¨¡å‹
            elif llm == online_model_name:
               with st.spinner('æ­£åœ¨ç”Ÿæˆå›ç­”...'):
                    response = M.ask_and_get_answer(online_model_name, 
                                            url=online_URL,
                                            vector_db=vector_store, 
                                            prompt=prompt, 
                                            template=DEFAULT_TEMPLATE,
                                            top_k=k,
                                            )
                    # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
                    with st.chat_message("assistant"):
                        st.markdown(response)
                    # æ·»åŠ å®Œæ•´å“åº”åˆ°å¯¹è¯å†å²
                    st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            st.warning('è¯·å…ˆæ·»åŠ æ•°æ®æˆ–ç­‰å¾…å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ', icon="âš ï¸")
    