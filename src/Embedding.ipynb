{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7245d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import  FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from LoadData import load_document,chunk_data,load_json\n",
    "\n",
    "from tool import skip_execution\n",
    "IS_SKIP =False\n",
    "\n",
    "embedding_name=\"BAAI/bge-small-zh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_emb_model(name):\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    # 下载 BAAI 官方的 BGE-Small 中文模型（自带 sentence_bert_config.json）\n",
    "    snapshot_download(\n",
    "        repo_id=name, \n",
    "        local_dir=os.path.join(project_dir,\"model\",name),\n",
    "        local_dir_use_symlinks=False,  # Windows 必加\n",
    "        allow_patterns=[\"*.json\", \"*.bin\", \"*.txt\", \"*.model\"] \n",
    "    )\n",
    "\n",
    "# download_emb_model(embedding_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf216d",
   "metadata": {},
   "source": [
    "https://huggingface.co/BAAI/bge-small-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding(embedding_name):\n",
    "    \"\"\"\n",
    "    根据embedding名称加载对应的嵌入模型\n",
    "    \"\"\"\n",
    "    # 通用模型参数配置\n",
    "    model_kwargs = {'device': 'cuda'}  \n",
    "    encode_kwargs = {'normalize_embeddings': True}  # 归一化嵌入向量\n",
    "    \n",
    "    embedding_path = os.path.join(project_dir,\"model\",embedding_name)\n",
    "    print(embedding_path)\n",
    "    \n",
    "    return HuggingFaceEmbeddings(\n",
    "            model_name=embedding_path,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aac657",
   "metadata": {},
   "source": [
    "* 闭源 API 模型\t: OpenAIEmbeddings\n",
    "  * OpenAI Ada-002、Anthropic Claude\t\t\n",
    "* 开源本地模型\t: HuggingFaceEmbeddings\n",
    "  * BERT、Sentence-BERT（如 all-MiniLM）\t\t\n",
    "* 云厂商模型\t: AliyunEmbeddings\n",
    "  * 阿里云通义千问嵌入、腾讯云向量嵌入\t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e838cb8",
   "metadata": {},
   "source": [
    "使用embedding模型持久化存储，目前常用的中文模型是bge-large-zh-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d959520",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_emb(): \n",
    "    embedding=get_embedding(embedding_name)\n",
    "          # 测试生成嵌入向量\n",
    "    test_text = \"这是一个测试句子，用于验证嵌入模型是否正常工作。\"\n",
    "    embedding_vector = embedding.embed_query(test_text)\n",
    "        \n",
    "        # 输出结果信息\n",
    "    print(f\"嵌入向量维度: {len(embedding_vector)}\")\n",
    "    print(f\"嵌入向量前5个值: {embedding_vector[:5]}\")\n",
    " \n",
    "    \n",
    "test_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6497bf0",
   "metadata": {},
   "source": [
    "### Chroma\n",
    "\n",
    "优势是以最小成本实现向量数据库的核心价值—— 无需关注底层细节，快速搭建可用的向量检索系统，尤其适合原型开发、中小规模应用或对部署复杂度敏感的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c687c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
    "def create_embeddings_chroma1(embedding_name, chunks, persist_dir=os.path.join(project_dir,\"db/chroma_db\")):\n",
    "    \"\"\"\n",
    "    创建并保存 Chroma 向量库\n",
    "    \"\"\"\n",
    "    # 获取嵌入模型\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    if not os.path.isdir(persist_dir):\n",
    "        os.mkdir(persist_dir)\n",
    "\n",
    "    # 创建向量库时指定保存路径\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir  # 指定本地保存目录\n",
    "    )\n",
    "    \n",
    "    # 打印保存信息\n",
    "    print(f\"Chroma 向量库已保存到: {os.path.abspath(persist_dir)}\")\n",
    "    return vector_store\n",
    "\n",
    "def load_embeddings_chroma(embedding_name, persist_dir):\n",
    "    \"\"\"\n",
    "    加载已保存的 Chroma 向量库\n",
    "    \"\"\"\n",
    "    # 获取与创建时相同的嵌入模型（必须一致，否则向量不兼容）\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    \n",
    "    # 加载本地向量库\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"Chroma 向量库已从 {os.path.abspath(persist_dir)} 加载\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29541e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "新版取消了vector_store.persist()\n",
    "'''\n",
    "# def create_embeddings_chroma(embedding_name, chunks, persist_dir=os.path.join(project_dir, \"db/chroma_db\"), batch_size=1000):\n",
    "#     embeddings = get_embedding(embedding_name)\n",
    "#     os.makedirs(persist_dir, exist_ok=True)\n",
    "    \n",
    "#     # 初始化Chroma（如果目录已存在则加载，实现断点续传）\n",
    "#     if os.path.exists(os.path.join(persist_dir, \"chroma.sqlite3\")):\n",
    "#         if vector_store ==None: \n",
    "#             vector_store = Chroma(\n",
    "#                 persist_directory=persist_dir,\n",
    "#                 embedding_function=embeddings\n",
    "#             )\n",
    "#         # 获取已存在的文档数量（用于断点续传）\n",
    "#         existing_count = vector_store._collection.count()\n",
    "#         print(f\"检测到已有向量库，已包含 {existing_count} 条文档\")\n",
    "#     else:\n",
    "#         # 新建向量库\n",
    "#         initial_batch = chunks[:batch_size]\n",
    "#         if not initial_batch:\n",
    "#             raise ValueError(\"❌ chunks 为空，无法初始化 Chroma 向量库\")\n",
    "\n",
    "#         vector_store = Chroma.from_documents(\n",
    "#             documents=initial_batch,\n",
    "#             embedding=embeddings,\n",
    "#             # persist_directory=persist_dir  # 这儿有bug\n",
    "#         )\n",
    "   \n",
    "#         existing_count = len(initial_batch)\n",
    "       \n",
    "#     # 边界检查：避免索引越界\n",
    "#     if existing_count >= len(chunks):\n",
    "#         print(\" 所有 chunks 已处理，无需更新 FAISS 向量库\")\n",
    "#         return vector_store\n",
    "#     # 从断点开始处理剩余文档\n",
    "#     remaining_chunks = chunks[existing_count:]\n",
    "\n",
    "#     print(len(remaining_chunks))\n",
    "#     total_batches = (len(remaining_chunks) + batch_size - 1) // batch_size\n",
    "    \n",
    "#     for i in range(total_batches):\n",
    "#         start = i * batch_size\n",
    "#         end = start + batch_size\n",
    "#         batch = remaining_chunks[start:end]\n",
    "        \n",
    "#         # 增量添加批次\n",
    "#         vector_store.add_documents(batch)\n",
    "        \n",
    "#         # 每10批保存一次（减少IO次数）\n",
    "#         if (i + 1) % 10 == 0:\n",
    "#             # vector_store.persist()\n",
    "#             print(f\"已处理 {start + end} 条文档，进度：{((i + 1) / total_batches) * 100:.2f}%\")\n",
    "    \n",
    " \n",
    "#     print(f\"Chroma 向量库已保存到: {os.path.abspath(persist_dir)}，共 {vector_store._collection.count()} 条文档\")\n",
    "#     return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a937e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/zzz/RAG_demo/datasets/tangshi.pdf\n",
      "352\n",
      "352\n",
      "/home/zzz/RAG_demo/model/BAAI/bge-small-zh\n",
      "342\n",
      "已处理 190 条文档，进度：28.57%\n",
      "已处理 390 条文档，进度：57.14%\n",
      "已处理 590 条文档，进度：85.71%\n",
      "Chroma 向量库已保存到: /home/zzz/RAG_demo/db/chroma_db，共 1166 条文档\n"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_chroma():\n",
    "    path = os.path.join(project_dir,\"datasets/tangshi.pdf\") \n",
    "    vector_path =os.path.join(project_dir,\"db/chroma_db\") \n",
    "\n",
    "    data = load_document(path)\n",
    "    chunks = chunk_data(data,chunk_size=512,chunk_overlap=100) \n",
    "    print(len(chunks))\n",
    "    create_embeddings_chroma(embedding_name,chunks,vector_path)\n",
    "    # load_embeddings_chroma(embedding_name,vector_path)\n",
    "test_chroma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d09ef",
   "metadata": {},
   "source": [
    "### Faiss\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. \n",
    "\n",
    "向量搜索算法库（纯工具库），专注于高维向量的高效相似性搜索，核心价值是提供优化的索引算法（如 IVF、HNSW、PQ 等），解决 “如何快速从海量向量中找到相似结果” 的技术问题\n",
    "\n",
    "https://faiss.ai/\n",
    "\n",
    "https://github.com/facebookresearch/faiss?tab=readme-ov-file\n",
    "\n",
    "* 暴力搜索（Brute-force）：\n",
    "索引类型：IndexFlatL2（L2 距离）、IndexFlatIP（内积，可用于余弦相似度）。\n",
    "特点：精确但速度慢，适合小规模数据（万级以下）。\n",
    "* 倒排文件索引（IVF）：\n",
    "索引类型：IndexIVFFlat、IndexIVFPQ等。\n",
    "特点：将向量聚类到多个桶（cluster），搜索时仅在目标桶内进行，平衡速度和精度，适合百万到亿级数据。\n",
    "* 分层导航小世界网络（HNSW）：\n",
    "索引类型：IndexHNSWFlat。\n",
    "特点：基于图结构的近似搜索，速度快、精度高，适合高维向量和实时场景。\n",
    "* 乘积量化（PQ）：\n",
    "索引类型：IndexPQ、IndexIVFPQ等。\n",
    "特点：将向量分段并量化，大幅降低内存占用，适合超大规模数据（十亿级）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embeddings_faiss( embedding_name, chunks,persist_dir=os.path.join(project_dir,\"db/faiss_db\") ):\n",
    "    \"\"\"\n",
    "    使用FAISS向量数据库，并保存\n",
    "    \"\"\"\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    if not os.path.isdir(persist_dir):\n",
    "        os.mkdir(persist_dir)\n",
    "\n",
    "    db.save_local(folder_path=persist_dir)\n",
    "    return db\n",
    "\n",
    "\n",
    "def load_embeddings_faiss( embedding_name,persist_dir):\n",
    "    \"\"\"\n",
    "    加载向量库\n",
    "    \"\"\"\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    db = FAISS.load_local(persist_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06a2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_embeddings_faiss(\n",
    "    embedding_name,\n",
    "    chunks,\n",
    "    persist_dir=os.path.join(project_dir, \"db/faiss_db\"),\n",
    "    batch_size=1000\n",
    "):\n",
    "    embeddings = get_embedding(embedding_name)\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    faiss_index_path = os.path.join(persist_dir, \"index.faiss\")\n",
    "    faiss_docstore_path = os.path.join(persist_dir, \"index.pkl\")\n",
    "\n",
    "    # 初始化 FAISS 向量库\n",
    "    if os.path.exists(faiss_index_path) and os.path.exists(faiss_docstore_path):\n",
    "        print(\"检测到已有 FAISS 向量库，尝试加载...\")\n",
    "        db = FAISS.load_local(persist_dir, embeddings, allow_dangerous_deserialization=True)\n",
    "        existing_count = len(db.docstore._dict)\n",
    "        print(f\"已加载 {existing_count} 条文档\")\n",
    "    else:\n",
    "        print(\"创建新的 FAISS 向量库...\")\n",
    "        initial_batch = chunks[:batch_size]\n",
    "        if not initial_batch:\n",
    "            raise ValueError(\" chunks 为空，无法初始化 FAISS 向量库\")\n",
    "        db = FAISS.from_documents(initial_batch, embeddings)\n",
    "        existing_count = len(initial_batch)\n",
    "\n",
    "    # 边界检查：避免索引越界\n",
    "    if existing_count >= len(chunks):\n",
    "        print(\" 所有 chunks 已处理，无需更新 FAISS 向量库\")\n",
    "        return db\n",
    "\n",
    "    remaining_chunks = chunks[existing_count:]\n",
    "    total_batches = math.ceil(len(remaining_chunks) / batch_size)\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(remaining_chunks))\n",
    "        batch = remaining_chunks[start:end]\n",
    "\n",
    "        if not batch:\n",
    "            print(f\"第 {i+1} 批为空，跳过\")\n",
    "            continue\n",
    "\n",
    "        print(f\"正在处理第 {i+1}/{total_batches} 批（{len(batch)} 条）\")\n",
    "\n",
    "        db.add_documents(batch)\n",
    "\n",
    "        # 每10批保存一次\n",
    "        if (i + 1) % 10 == 0 or i == total_batches - 1:\n",
    "            db.save_local(persist_dir)\n",
    "            print(f\"已保存至 {persist_dir}，当前总文档数：{len(db.docstore._dict)}\")\n",
    "\n",
    "    print(f\"FAISS 向量库构建完成，共 {len(db.docstore._dict)} 条文档\")\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce40412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/zzz/RAG_demo/datasets/tangshi.pdf\n",
      "352\n",
      "/home/zzz/RAG_demo/model/BAAI/bge-small-zh\n",
      "检测到已有 FAISS 向量库，尝试加载...\n",
      "已加载 352 条文档\n",
      " 所有 chunks 已处理，无需更新 FAISS 向量库\n",
      "/home/zzz/RAG_demo/model/BAAI/bge-small-zh\n"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_faiss():\n",
    "    path = os.path.join(project_dir,\"datasets/tangshi.pdf\") \n",
    "    vector_path =os.path.join(project_dir,\"db/faiss_db\") \n",
    "    data = load_document(path)\n",
    "    chunks = chunk_data(data,chunk_size=512,chunk_overlap=100) \n",
    "    create_embeddings_faiss(embedding_name,chunks,vector_path,100)\n",
    "    load_embeddings_faiss(embedding_name,vector_path)\n",
    "    \n",
    "test_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57502f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_law():\n",
    "    path = os.path.join(project_dir,\"datasets\",\"chinese_law_ft_dataset.json\") \n",
    "    vector_path =os.path.join(project_dir,\"db/law_db\") \n",
    "    data = load_json(path)\n",
    "    chunks = chunk_data(data,chunk_size=512,chunk_overlap=100) \n",
    "    create_embeddings_faiss(embedding_name,chunks,vector_path,batch_size=100)\n",
    "    # load_embeddings_chroma(embedding_name,vector_path)\n",
    "    \n",
    "test_law()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
