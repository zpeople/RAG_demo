{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3adcc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import time\n",
    "from typing import Dict, Any, Mapping, Optional, List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "from vllm import LLM as VLLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from tool import skip_execution\n",
    "from Embedding import load_embeddings_faiss,load_embeddings_chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301f92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME =\"Qwen/Qwen3-0.6B\"\n",
    "IS_SKIP=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e49d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"输入: {prompt}\")\n",
    "        print(f\"输出: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def download_model(localpath,modelname):\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=modelname, \n",
    "        local_dir=localpath,\n",
    "        local_dir_use_symlinks=False,  # Windows 必加\n",
    "        allow_patterns=[\"*.json\", \"*.bin\", \"*.txt\", \"*.model\"] \n",
    "    )\n",
    "    \n",
    "def get_llm_model(\n",
    "        prompt: str = None,\n",
    "        model_name: str = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_token: int = 2048,\n",
    "        n_ctx: int = 512):\n",
    "    \"\"\"\n",
    "    根据模型名称去加载模型，返回response数据\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(project_dir,\"model\",model_name)\n",
    "    print(model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        download_model(model_path,model_name)\n",
    "\n",
    "    # 配置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_token,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # 初始化VLLM\n",
    "    llm = VLLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=1,  # 根据GPU数量调整\n",
    "        gpu_memory_utilization=0.8,\n",
    "        max_num_batched_tokens=n_ctx,\n",
    "        max_model_len=n_ctx,\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    # 生成响应\n",
    "    response = llm.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个智能超级助手，请用专业的词语回答问题，整体上下文带有逻辑性，如果不知道，请不要乱说\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    cost = time.time() - start\n",
    "    print(f\"模型生成时间：{cost}\")\n",
    "    print(f\"大模型回复：\\n{response}\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66318b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_get_llm():\n",
    "    outputs =get_llm_model(\"你是谁\",MODEL_NAME,0.8,1024,512)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "# test_get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f201a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenLLM(LLM):\n",
    "    \"\"\"\n",
    "    基于VLLM的自定义QwenLLM\n",
    "    \"\"\"\n",
    "    model_name: str = \"\"\n",
    "    # 访问时延上限\n",
    "    request_timeout: float = None\n",
    "    # 温度系数\n",
    "    temperature: float = 0.8\n",
    "    # 窗口大小\n",
    "    n_ctx :int =2048\n",
    "    # token大小\n",
    "    max_tokens:int= 1024\n",
    "    # 并行计算数量\n",
    "    tensor_parallel_size: int = 1\n",
    "    # 模型参数\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    # VLLM实例\n",
    "    _llm: Optional[VLLM] = None\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(** data)\n",
    "        self._initialize_llm()\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"初始化VLLM实例\"\"\"\n",
    "        model_path = os.path.join(project_dir,\"model\",self.model_name)\n",
    "        print(\"qwen_path:\", model_path)\n",
    "        \n",
    "        self._llm = VLLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=self.tensor_parallel_size,\n",
    "            gpu_memory_utilization=0.8,\n",
    "            max_num_batched_tokens=self.n_ctx,\n",
    "            max_model_len=self.n_ctx,\n",
    "            **self.model_kwargs\n",
    "        )\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              ** kwargs: Any):\n",
    "        # 配置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            top_p=0.95,\n",
    "            stop=stop or []\n",
    "        )\n",
    "        \n",
    "        # 生成响应\n",
    "        response = self._llm.chat(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "\n",
    "        print(f\"Qwen response: \\n{response}\")\n",
    "        return response[0].outputs[0].text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vllm-qwen\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"request_timeout\": self.request_timeout,\n",
    "            \"n_ctx\": self.n_ctx,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"tensor_parallel_size\": self.tensor_parallel_size\n",
    "        }\n",
    "        return {**normal_params, **self.model_kwargs}\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model_name\": self.model_name}, **self._default_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e75a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_with_langchain():\n",
    "    # 初始化模型\n",
    "    llm = QwenLLM(\n",
    "        model_name=MODEL_NAME,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    # 创建一个简单的链\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"请回答以下问题: {question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # 运行测试\n",
    "    result = chain.run(\"什么是机器学习？\")\n",
    "    print(f\"LangChain测试结果: {result}\")\n",
    "\n",
    "\n",
    "# test_with_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8314d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 从标志结束的位置开始截取\n",
    "def extract_after_flag(text, flag):\n",
    "    # 找到标志的起始索引\n",
    "    index = text.find(flag)\n",
    "    if index != -1:\n",
    "        return text[index + len(flag):]\n",
    "    return \"\"  # 如果没有找到标志，返回空字符串\n",
    "\n",
    "\n",
    "\n",
    "def ask_and_get_answer_from_local(model_name, vector_db, prompt,template, top_k=5):\n",
    "    \"\"\"\n",
    "    从本地加载大模型\n",
    "    :param model_name: 模型名称\n",
    "    :param vector_db:\n",
    "    :param prompt:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    llm = QwenLLM(model_name=model_name, temperature=0.4)\n",
    "    if not IS_SKIP:#  创建基础提示模板（无上下文） 直接生成回答 测试的时候用来对比输出\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=template.replace(\"{context}\\n\", \"\")  # 移除上下文占位符\n",
    "        )\n",
    "        \n",
    "        prompt_text = prompt_template.format(question=prompt)\n",
    "        direct_answer = llm(prompt_text)  \n",
    "        print(f\"direct answers: {direct_answer}\")\n",
    "\n",
    "    # RAG \n",
    "    docs_and_scores = vector_db.similarity_search_with_score(prompt, k=top_k)\n",
    "    print(\"docs_and_scores: \", docs_and_scores)\n",
    "    knowledge = [doc[\"page_content\"] for doc in docs_and_scores]\n",
    "    print(\"检索到的知识：\", knowledge)\n",
    "\n",
    "    prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    retriever = vector_db.as_retriever(search_type='similarity', search_kwargs={'k': top_k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=retriever,\n",
    "                                        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "                                        return_source_documents=True)\n",
    "    answer = chain({\"query\": prompt, \"top_k\": top_k})\n",
    "    # answer = chain.run(prompt)\n",
    "    # answer = answer['choices'][0]['message']['content']\n",
    "    answer = answer['result']\n",
    "    print(f\"answers: {answer}\")\n",
    "    answer =extract_after_flag(answer,\"</think>\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7120721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 test_getRagAnswer_chroma 已跳过执行\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"\n",
    "        你是一个聪明的超级智能助手，请用专业且富有逻辑顺序的句子回复，并以中文形式且markdown形式输出。\n",
    "        检索到的信息：\n",
    "        {context}\n",
    "        问题：\n",
    "        {question}\n",
    "    \"\"\"\n",
    "prompt =\"'少小离家老大回'的下一句诗是什么\"\n",
    "\n",
    "embedding_name =\"BAAI/bge-small-zh\"\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_faiss():\n",
    "    faiss_db = load_embeddings_faiss(embedding_name,vector_db_path=os.path.join(project_dir,\"db/faiss_db\"))\n",
    "    ask_and_get_answer_from_local(MODEL_NAME,faiss_db,prompt,DEFAULT_TEMPLATE)\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_chroma():\n",
    "    chroma_db = load_embeddings_chroma(embedding_name,persist_dir=os.path.join(project_dir,\"db/chroma_db\"))\n",
    "    ask_and_get_answer_from_local(MODEL_NAME,chroma_db,prompt,DEFAULT_TEMPLATE)\n",
    "\n",
    "test_getRagAnswer_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f9d332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-04 08:46:57] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: /home/zzz/RAG_demo/model/BAAI/bge-small-zh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zzz/RAG_demo/model/BAAI/bge-small-zh\n",
      "Chroma 向量库已从 /home/zzz/RAG_demo/db/law_db 加载\n",
      "qwen_path: /home/zzz/RAG_demo/model/Qwen/Qwen3-0.6B\n",
      "INFO 09-04 08:46:57 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-04 08:46:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 09-04 08:46:57 [config.py:1443] Possibly too large swap space. 4.00 GiB out of the 7.59 GiB total CPU memory is allocated for the swap space.\n",
      "INFO 09-04 08:47:02 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 09-04 08:47:05 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/zzz/RAG_demo/model/Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='/home/zzz/RAG_demo/model/Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/zzz/RAG_demo/model/Qwen/Qwen3-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 09-04 08:47:06 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa0ff9145b0>\n",
      "INFO 09-04 08:47:07 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "WARNING 09-04 08:47:07 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 09-04 08:47:07 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 09-04 08:47:07 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 09-04 08:47:07 [gpu_model_runner.py:1329] Starting to load model /home/zzz/RAG_demo/model/Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.57s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 08:47:09 [loader.py:458] Loading weights took 1.58 seconds\n",
      "INFO 09-04 08:47:09 [gpu_model_runner.py:1347] Model loading took 1.1201 GiB and 1.882686 seconds\n",
      "INFO 09-04 08:47:17 [backends.py:420] Using cache directory: /home/zzz/.cache/vllm/torch_compile_cache/f627082f4d/rank_0_0 for vLLM's torch.compile\n",
      "INFO 09-04 08:47:17 [backends.py:430] Dynamo bytecode transform time: 7.78 s\n",
      "INFO 09-04 08:47:21 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 4.167 s\n",
      "INFO 09-04 08:47:22 [monitor.py:33] torch.compile takes 7.78 s in total\n",
      "INFO 09-04 08:47:25 [kv_cache_utils.py:634] GPU KV cache size: 25,296 tokens\n",
      "INFO 09-04 08:47:25 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 12.35x\n",
      "INFO 09-04 08:47:47 [gpu_model_runner.py:1686] Graph capturing finished in 24 secs, took 0.45 GiB\n",
      "INFO 09-04 08:47:47 [core.py:159] init engine (profile, create kv cache, warmup model) took 38.27 seconds\n",
      "INFO 09-04 08:47:47 [core_client.py:439] Core engine process 0 ready.\n",
      "docs_and_scores:  [(Document(id='19da1e5e-9ee2-41d9-a4c5-83ea49f9e8fa', metadata={'id': 271550}, page_content=\"{'user_query': '杀人自首后是否会减轻刑罚，具体情况如何？', 'answer': '杀人自首后可以从轻或减轻刑罚。根据我国刑法规定，杀人罪是一种严重的犯罪，最高可判处死刑。但是，如果犯罪嫌疑人杀人后自首，并如实供述自己的罪行，可以在量刑时考虑减轻或从轻处罚。具体的刑罚量刑还需要综合考虑犯罪嫌疑人的个人情况、犯罪的性质、情节、后果等因素。需要注意的是，自首只是一种情节，不能完全免除犯罪嫌疑人的刑事责任。', 'legal_basis': []}\"), 0.26899364590644836), (Document(id='42e8aac8-a16e-4ed6-b901-340fe4bb5f91', metadata={'id': 124005}, page_content=\"'answer': '适用法条：\\\\n 《中华人民共和国刑法》第二百三十二条 +【故意杀人罪】+故意杀人的，处死刑、无期徒刑或者十年以上有期徒刑;情节较轻的，处三年以上十年以下有期徒刑。第六十七条 【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。', 'legal_basis': []}\"), 0.2819153964519501), (Document(id='a7bdfb1c-cb6c-4221-afb4-d5de39bf53ec', metadata={'id': 116126}, page_content=\"+【自首】+犯罪嫌疑人虽不具有自首情节，但是如实供述自己罪行的，可以从轻处罚;因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。\\\\n    判决结果：\\\\n  罪名：故意杀人罪 刑期：有期徒刑三年以下，有减轻刑罚情节。', 'legal_basis': []}\"), 0.28881174325942993), (Document(id='03db2579-3cb0-4004-ab32-512feb83c17f', metadata={'id': 117747}, page_content=\"《中华人民共和国刑法》第六十七条+【自首】+被告人在案发后自动投案，并如实供述罪行，属于自首情节，可从轻处罚。 \\\\n    判决结果：\\\\n 罪名：故意杀人罪刑期：三年以上，十年以下有期徒刑。注：被告人自首和立功情节虽然可以从轻处罚，但是由于罪行严重，刑期仍然较长。', 'legal_basis': []}\"), 0.2931967079639435), (Document(id='6086c493-4204-4d28-ae37-d9a7235b13d6', metadata={'id': 114929}, page_content=\"适用法条：\\\\n 《中华人民共和国刑法》第二百三十二条 +【故意杀人罪】+故意杀人的，处死刑、无期徒刑或者十年以上有期徒刑;情节较轻的，处三年以上十年以下有期徒刑。第六十七条 【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。\\\\n    判决结果：\\\\n 罪名：罪名：故意杀人罪刑期：有期徒刑三年', 'legal_basis': []}\"), 0.3051064610481262)]\n",
      "检索到的知识： [(Document(id='19da1e5e-9ee2-41d9-a4c5-83ea49f9e8fa', metadata={'id': 271550}, page_content=\"{'user_query': '杀人自首后是否会减轻刑罚，具体情况如何？', 'answer': '杀人自首后可以从轻或减轻刑罚。根据我国刑法规定，杀人罪是一种严重的犯罪，最高可判处死刑。但是，如果犯罪嫌疑人杀人后自首，并如实供述自己的罪行，可以在量刑时考虑减轻或从轻处罚。具体的刑罚量刑还需要综合考虑犯罪嫌疑人的个人情况、犯罪的性质、情节、后果等因素。需要注意的是，自首只是一种情节，不能完全免除犯罪嫌疑人的刑事责任。', 'legal_basis': []}\"), 0.26899364590644836), (Document(id='42e8aac8-a16e-4ed6-b901-340fe4bb5f91', metadata={'id': 124005}, page_content=\"'answer': '适用法条：\\\\n 《中华人民共和国刑法》第二百三十二条 +【故意杀人罪】+故意杀人的，处死刑、无期徒刑或者十年以上有期徒刑;情节较轻的，处三年以上十年以下有期徒刑。第六十七条 【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。', 'legal_basis': []}\"), 0.2819153964519501), (Document(id='a7bdfb1c-cb6c-4221-afb4-d5de39bf53ec', metadata={'id': 116126}, page_content=\"+【自首】+犯罪嫌疑人虽不具有自首情节，但是如实供述自己罪行的，可以从轻处罚;因其如实供述自己罪行，避免特别严重后果发生的，可以减轻处罚。\\\\n    判决结果：\\\\n  罪名：故意杀人罪 刑期：有期徒刑三年以下，有减轻刑罚情节。', 'legal_basis': []}\"), 0.28881174325942993), (Document(id='03db2579-3cb0-4004-ab32-512feb83c17f', metadata={'id': 117747}, page_content=\"《中华人民共和国刑法》第六十七条+【自首】+被告人在案发后自动投案，并如实供述罪行，属于自首情节，可从轻处罚。 \\\\n    判决结果：\\\\n 罪名：故意杀人罪刑期：三年以上，十年以下有期徒刑。注：被告人自首和立功情节虽然可以从轻处罚，但是由于罪行严重，刑期仍然较长。', 'legal_basis': []}\"), 0.2931967079639435), (Document(id='6086c493-4204-4d28-ae37-d9a7235b13d6', metadata={'id': 114929}, page_content=\"适用法条：\\\\n 《中华人民共和国刑法》第二百三十二条 +【故意杀人罪】+故意杀人的，处死刑、无期徒刑或者十年以上有期徒刑;情节较轻的，处三年以上十年以下有期徒刑。第六十七条 【自首】犯罪以后自动投案，如实供述自己的罪行的，是自首。对于自首的犯罪分子，可以从轻或者减轻处罚。其中，犯罪较轻的，可以免除处罚。\\\\n    判决结果：\\\\n 罪名：罪名：故意杀人罪刑期：有期徒刑三年', 'legal_basis': []}\"), 0.3051064610481262)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1614/1197858847.py:44: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = chain({\"query\": prompt, \"top_k\": top_k})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 08:47:50 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150cee08181242daa0ad1ee12916bf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen response: \n",
      "[RequestOutput(request_id=0, prompt=None, prompt_token_ids=[151644, 8948, 198, 56568, 101909, 100168, 104453, 110498, 37945, 11622, 58, 104811, 60, 104715, 113042, 102104, 86119, 3837, 101932, 102285, 16744, 106646, 104913, 33071, 90395, 23031, 60073, 68805, 66017, 151645, 198, 151644, 872, 271, 286, 220, 56568, 101909, 105414, 9370, 104453, 100168, 110498, 37945, 11622, 99878, 100136, 106573, 104913, 107684, 9370, 109949, 104787, 90395, 23031, 104811, 100414, 100136, 60073, 100414, 66017, 8997, 286, 6567, 96, 222, 50984, 26939, 105427, 28311, 286, 5360, 872, 5738, 1210, 364, 107642, 35926, 59975, 33447, 113985, 106104, 99867, 100554, 3837, 111142, 100007, 11319, 516, 364, 9217, 1210, 364, 107642, 35926, 59975, 33447, 112255, 99578, 57191, 106104, 99867, 100554, 1773, 100345, 101055, 111849, 99812, 3837, 107642, 100426, 101158, 105806, 101218, 3837, 103994, 30440, 112217, 115491, 1773, 100131, 3837, 62244, 108793, 17340, 107642, 33447, 35926, 59975, 90395, 115970, 83744, 41932, 100005, 100426, 22243, 3837, 104964, 32757, 99867, 13343, 101118, 106104, 57191, 45181, 99578, 102520, 1773, 108247, 99867, 100554, 32757, 99867, 106750, 99799, 101118, 108793, 103947, 99605, 99559, 5373, 101218, 9370, 105155, 5373, 102892, 5373, 105691, 110276, 1773, 107916, 100146, 3837, 35926, 59975, 91680, 101158, 102892, 3837, 53153, 100372, 118357, 108793, 103947, 117782, 1773, 516, 364, 6428, 62696, 1210, 3056, 630, 6, 9217, 1210, 364, 105255, 24339, 38989, 5122, 59, 77, 220, 26940, 105492, 104773, 111849, 25067, 99802, 99271, 44991, 115517, 488, 10904, 105830, 107642, 100426, 10958, 10, 105830, 99733, 103947, 3837, 44290, 115491, 5373, 42192, 22704, 100273, 99867, 100631, 100499, 70589, 112051, 26, 102892, 99260, 99578, 9370, 3837, 44290, 101256, 70589, 100499, 87752, 112051, 1773, 102651, 118263, 33576, 35926, 59975, 10958, 101218, 103934, 100756, 79072, 80642, 3837, 115970, 83744, 41932, 100005, 100426, 22243, 9370, 3837, 20412, 35926, 59975, 1773, 100002, 35926, 59975, 9370, 101218, 102388, 3837, 112255, 99578, 100631, 106104, 102520, 1773, 90919, 3837, 101218, 99260, 99578, 9370, 3837, 73670, 118357, 102520, 1773, 516, 364, 6428, 62696, 1210, 3056, 630, 10, 10904, 35926, 59975, 10958, 10, 108793, 17340, 100628, 16530, 100629, 35926, 59975, 102892, 3837, 100131, 115970, 83744, 41932, 99283, 100426, 22243, 9370, 3837, 112255, 99578, 102520, 26, 111615, 115970, 83744, 41932, 99283, 100426, 22243, 3837, 101153, 100654, 101120, 105691, 106806, 3837, 73670, 106104, 102520, 66898, 77, 262, 19468, 97, 99351, 59151, 5122, 59, 77, 220, 10236, 121, 103, 13072, 5122, 105830, 107642, 100426, 19468, 239, 22704, 5122, 112051, 101256, 87752, 3837, 18830, 106104, 99867, 100554, 102892, 1773, 516, 364, 6428, 62696, 1210, 3056, 630, 26940, 105492, 104773, 111849, 25067, 102651, 118263, 10, 10904, 35926, 59975, 10958, 10, 103051, 106164, 80642, 28291, 33447, 100756, 79072, 80642, 90395, 115970, 83744, 41932, 100426, 22243, 3837, 100409, 35926, 59975, 102892, 3837, 30440, 45181, 99578, 102520, 1773, 1124, 77, 262, 19468, 97, 99351, 59151, 5122, 59, 77, 10236, 121, 103, 13072, 5122, 105830, 107642, 100426, 99867, 22704, 5122, 101256, 70589, 3837, 100499, 87752, 112051, 1773, 25074, 5122, 108360, 35926, 59975, 33108, 79095, 17404, 102892, 103925, 112255, 99578, 102520, 3837, 100131, 101887, 100426, 22243, 101120, 3837, 99867, 22704, 104187, 112228, 1773, 516, 364, 6428, 62696, 1210, 3056, 630, 105255, 24339, 38989, 5122, 59, 77, 220, 26940, 105492, 104773, 111849, 25067, 99802, 99271, 44991, 115517, 488, 10904, 105830, 107642, 100426, 10958, 10, 105830, 99733, 103947, 3837, 44290, 115491, 5373, 42192, 22704, 100273, 99867, 100631, 100499, 70589, 112051, 26, 102892, 99260, 99578, 9370, 3837, 44290, 101256, 70589, 100499, 87752, 112051, 1773, 102651, 118263, 33576, 35926, 59975, 10958, 101218, 103934, 100756, 79072, 80642, 3837, 115970, 83744, 41932, 100005, 100426, 22243, 9370, 3837, 20412, 35926, 59975, 1773, 100002, 35926, 59975, 9370, 101218, 102388, 3837, 112255, 99578, 100631, 106104, 102520, 1773, 90919, 3837, 101218, 99260, 99578, 9370, 3837, 73670, 118357, 102520, 66898, 77, 262, 19468, 97, 99351, 59151, 5122, 59, 77, 10236, 121, 103, 13072, 5122, 100426, 13072, 5122, 105830, 107642, 100426, 99867, 22704, 5122, 112051, 101256, 516, 364, 6428, 62696, 1210, 3056, 532, 286, 220, 86119, 28311, 286, 60596, 222, 17340, 35926, 59975, 103934, 3837, 99494, 99713, 99867, 198, 257, 151645, 198, 151644, 77091, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='<think>\\n好的，用户的问题是关于杀人自首后如何判刑，我需要先回顾之前提供的检索信息。用户已经检索到多个相关答案，但需要整合成一个连贯的Markdown格式回答。\\n\\n首先，我需要确定用户的需求。他们想知道自首后的刑罚情况，可能需要了解法律依据和具体判决结果。根据提供的信息，自首可以减轻或免除处罚，但刑期可能更重。需要确保回答结构清晰，使用专业术语，同时保持逻辑顺序。\\n\\n接下来，我需要检查每个法律依据是否正确引用，比如《刑法》第二百三十二条和第六十七条。同时，注意不同回答中的判决结果是否一致，避免混淆。最后，确保回答符合中文专业表达，没有使用Markdown格式，而是用自然的中文段落呈现。\\n</think>\\n\\n根据我国《中华人民共和国刑法》相关规定，**杀人自首后**的判刑情况如下：  \\n\\n1. **适用法条**：  \\n   - 《刑法》第二百三十二条（故意杀人罪）  \\n   - 《刑法》第六十七条（自首情节）  \\n\\n2. **自首认定**：  \\n   - 犯罪嫌疑人若在案发后**自动投案并如实供述**罪行，且**未有自首情节**，但**如实供述**可**减轻处罚**；若因如实供述**避免特别严重后果**，可**减轻处罚**。  \\n\\n3. **判刑结果**：  \\n   - **自首情节**可**从轻或减轻处罚**，具体刑期根据犯罪性质、情节及后果综合决定。  \\n   - **无自首情节**但**如实供述**的，可**判处三年以下有期徒刑**；若犯罪情节严重，刑期可**三年以上十年以下**。  \\n\\n**注**：自首仅作为量刑情节，**不能免除刑事责任**，且因罪行严重，刑期仍需依法确定。', token_ids=[151667, 198, 99692, 3837, 20002, 103936, 20412, 101888, 107642, 35926, 59975, 33447, 100007, 99713, 99867, 3837, 35946, 85106, 60726, 106113, 101056, 103008, 116928, 27369, 1773, 20002, 99461, 116928, 26939, 101213, 78556, 102349, 3837, 77288, 85106, 102102, 12857, 46944, 54926, 100116, 9370, 68005, 68805, 102104, 3407, 101140, 3837, 35946, 85106, 60610, 20002, 104378, 1773, 99650, 109623, 35926, 59975, 104813, 99867, 100554, 99559, 3837, 87267, 85106, 99794, 100376, 104282, 33108, 100398, 106530, 59151, 1773, 100345, 103008, 27369, 3837, 35926, 59975, 73670, 106104, 57191, 118357, 102520, 3837, 77288, 99867, 22704, 87267, 33126, 29258, 1773, 85106, 103944, 102104, 100166, 104542, 3837, 37029, 99878, 116925, 3837, 91572, 100662, 104913, 107684, 3407, 104326, 3837, 35946, 85106, 101071, 103991, 100376, 104282, 64471, 88991, 103346, 3837, 101912, 26940, 111849, 25067, 99802, 99271, 44991, 115517, 33108, 102651, 118263, 1773, 91572, 3837, 60533, 99604, 102104, 101047, 106530, 59151, 64471, 101266, 3837, 101153, 118943, 1773, 100161, 3837, 103944, 102104, 101137, 104811, 99878, 102124, 3837, 80443, 37029, 68005, 68805, 3837, 103955, 11622, 99795, 9370, 104811, 37474, 99297, 104401, 8997, 151668, 271, 100345, 101055, 26940, 105492, 104773, 111849, 25067, 110505, 3837, 334, 107642, 35926, 59975, 33447, 334, 9370, 99713, 99867, 99559, 104506, 5122, 18611, 16, 13, 3070, 105255, 24339, 38989, 334, 5122, 2303, 256, 481, 220, 26940, 111849, 25067, 99802, 99271, 44991, 115517, 9909, 105830, 107642, 100426, 7552, 2303, 256, 481, 220, 26940, 111849, 25067, 102651, 118263, 9909, 35926, 59975, 102892, 7552, 18611, 17, 13, 3070, 35926, 59975, 104585, 334, 5122, 2303, 256, 481, 10236, 232, 107, 100426, 111769, 100155, 18493, 80642, 28291, 33447, 334, 100756, 79072, 80642, 62926, 115970, 83744, 41932, 334, 100426, 22243, 3837, 100136, 334, 38342, 18830, 35926, 59975, 102892, 334, 3837, 77288, 334, 115970, 83744, 41932, 334, 30440, 334, 106104, 102520, 334, 24968, 100155, 62112, 115970, 83744, 41932, 334, 101153, 100654, 101120, 105691, 334, 3837, 30440, 334, 106104, 102520, 334, 1773, 18611, 18, 13, 3070, 99713, 99867, 59151, 334, 5122, 2303, 256, 481, 3070, 35926, 59975, 102892, 334, 30440, 334, 45181, 99578, 57191, 106104, 102520, 334, 3837, 100398, 99867, 22704, 100345, 101218, 105155, 5373, 102892, 81217, 105691, 99799, 103930, 1773, 2303, 256, 481, 3070, 42192, 35926, 59975, 102892, 334, 77288, 334, 115970, 83744, 41932, 334, 9370, 3837, 30440, 334, 112217, 101256, 87752, 112051, 334, 24968, 100155, 101218, 102892, 101120, 3837, 99867, 22704, 30440, 334, 101256, 70589, 100499, 87752, 334, 1773, 18611, 334, 25074, 334, 5122, 35926, 59975, 99373, 100622, 32757, 99867, 102892, 3837, 334, 53153, 118357, 117782, 334, 3837, 100136, 62112, 100426, 22243, 101120, 3837, 99867, 22704, 99754, 58362, 102076, 60610, 1773, 151645], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]\n",
      "answers: <think>\n",
      "好的，用户的问题是关于杀人自首后如何判刑，我需要先回顾之前提供的检索信息。用户已经检索到多个相关答案，但需要整合成一个连贯的Markdown格式回答。\n",
      "\n",
      "首先，我需要确定用户的需求。他们想知道自首后的刑罚情况，可能需要了解法律依据和具体判决结果。根据提供的信息，自首可以减轻或免除处罚，但刑期可能更重。需要确保回答结构清晰，使用专业术语，同时保持逻辑顺序。\n",
      "\n",
      "接下来，我需要检查每个法律依据是否正确引用，比如《刑法》第二百三十二条和第六十七条。同时，注意不同回答中的判决结果是否一致，避免混淆。最后，确保回答符合中文专业表达，没有使用Markdown格式，而是用自然的中文段落呈现。\n",
      "</think>\n",
      "\n",
      "根据我国《中华人民共和国刑法》相关规定，**杀人自首后**的判刑情况如下：  \n",
      "\n",
      "1. **适用法条**：  \n",
      "   - 《刑法》第二百三十二条（故意杀人罪）  \n",
      "   - 《刑法》第六十七条（自首情节）  \n",
      "\n",
      "2. **自首认定**：  \n",
      "   - 犯罪嫌疑人若在案发后**自动投案并如实供述**罪行，且**未有自首情节**，但**如实供述**可**减轻处罚**；若因如实供述**避免特别严重后果**，可**减轻处罚**。  \n",
      "\n",
      "3. **判刑结果**：  \n",
      "   - **自首情节**可**从轻或减轻处罚**，具体刑期根据犯罪性质、情节及后果综合决定。  \n",
      "   - **无自首情节**但**如实供述**的，可**判处三年以下有期徒刑**；若犯罪情节严重，刑期可**三年以上十年以下**。  \n",
      "\n",
      "**注**：自首仅作为量刑情节，**不能免除刑事责任**，且因罪行严重，刑期仍需依法确定。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W904 08:47:54.462638854 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "prompt =\"杀人自首以后，怎么判刑\"\n",
    "# @skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_chroma():\n",
    "    chroma_db = load_embeddings_chroma(embedding_name,persist_dir=os.path.join(project_dir,\"db/law_db\"))\n",
    "    ask_and_get_answer_from_local(MODEL_NAME,chroma_db,prompt,DEFAULT_TEMPLATE)\n",
    "\n",
    "test_getRagAnswer_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bf469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
