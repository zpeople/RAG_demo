{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Any, Mapping, Optional, List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "from vllm import LLM as VLLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from tool import skip_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME =\"Qwen/Qwen3-0.6B\"\n",
    "IS_SKIP=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"输入: {prompt}\")\n",
    "        print(f\"输出: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def download_model(localpath,modelname):\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=modelname, \n",
    "        local_dir=localpath,\n",
    "        local_dir_use_symlinks=False,  # Windows 必加\n",
    "        allow_patterns=[\"*.json\", \"*.bin\", \"*.txt\", \"*.model\"] \n",
    "    )\n",
    "    \n",
    "def get_llm_model(\n",
    "        prompt: str = None,\n",
    "        model: str = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_token: int = 2048,\n",
    "        n_ctx: int = 512):\n",
    "    \"\"\"\n",
    "    根据模型名称去加载模型，返回response数据\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(\"../model\",model)\n",
    "    print(model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        download_model(model_path,model)\n",
    "\n",
    "    # 配置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_token,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # 初始化VLLM\n",
    "    llm = VLLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=1,  # 根据GPU数量调整\n",
    "        gpu_memory_utilization=0.9,\n",
    "        max_num_batched_tokens=n_ctx,\n",
    "        max_model_len=10000,\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    # 生成响应\n",
    "    response = llm.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个智能超级助手，请用专业的词语回答问题，整体上下文带有逻辑性，如果不知道，请不要乱说\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    cost = time.time() - start\n",
    "    print(f\"模型生成时间：{cost}\")\n",
    "    print(f\"大模型回复：\\n{response}\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66318b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_get_llm():\n",
    "    outputs =get_llm_model(\"你是谁\",MODEL_NAME,0.8,1024,512)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "test_get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f201a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenLLM(LLM):\n",
    "    \"\"\"\n",
    "    基于VLLM的自定义QwenLLM\n",
    "    \"\"\"\n",
    "    model_name: str = \"\"\n",
    "    # 访问时延上限\n",
    "    request_timeout: float = None\n",
    "    # 温度系数\n",
    "    temperature: float = 0.8\n",
    "    # 窗口大小\n",
    "    n_ctx :int =2048\n",
    "    # token大小\n",
    "    max_tokens:int= 1024\n",
    "    # 并行计算数量\n",
    "    tensor_parallel_size: int = 1\n",
    "    # 模型参数\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    # VLLM实例\n",
    "    _llm: Optional[VLLM] = None\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(** data)\n",
    "        self._initialize_llm()\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"初始化VLLM实例\"\"\"\n",
    "        model_path = os.path.join(\"../model\",self.model_name)\n",
    "        print(\"qwen_path:\", model_path)\n",
    "        \n",
    "        self._llm = VLLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=self.tensor_parallel_size,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_num_batched_tokens=self.n_ctx,\n",
    "            max_model_len=10000,\n",
    "            **self.model_kwargs\n",
    "        )\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              ** kwargs: Any):\n",
    "        # 配置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            top_p=0.95,\n",
    "            stop=stop or []\n",
    "        )\n",
    "        \n",
    "        # 生成响应\n",
    "        response = self._llm.chat(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "\n",
    "        print(f\"Qwen response: \\n{response}\")\n",
    "        return response[0].outputs[0].text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vllm-qwen\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"request_timeout\": self.request_timeout,\n",
    "            \"n_ctx\": self.n_ctx,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"tensor_parallel_size\": self.tensor_parallel_size\n",
    "        }\n",
    "        return {**normal_params, **self.model_kwargs}\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model_name\": self.model_name}, **self._default_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_with_langchain():\n",
    "    # 初始化模型\n",
    "    llm = QwenLLM(\n",
    "        model_name=MODEL_NAME,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    # 创建一个简单的链\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"请回答以下问题: {question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # 运行测试\n",
    "    result = chain.run(\"什么是机器学习？\")\n",
    "    print(f\"LangChain测试结果: {result}\")\n",
    "\n",
    "\n",
    "test_with_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8314d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"\n",
    "    你是一个聪明的超级智能助手，请用专业且富有逻辑顺序的句子回复，并以中文形式且markdown形式输出。\n",
    "    检索到的信息：\n",
    "    {context}\n",
    "    问题：\n",
    "    {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def ask_and_get_answer_from_local(model_name, vector_db, prompt, top_k=5):\n",
    "    \"\"\"\n",
    "    从本地加载大模型\n",
    "    :param model_name: 模型名称\n",
    "    :param vector_db:\n",
    "    :param prompt:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    docs_and_scores = vector_db.similarity_search_with_score(prompt, k=top_k)\n",
    "    print(\"docs_and_scores: \", docs_and_scores)\n",
    "    # knowledge = [doc.page_content for doc in docs_and_scores]\n",
    "    # print(\"检索到的知识：\", knowledge)\n",
    "    llm = QwenLLM(model_name=model_name, temperature=0.4)\n",
    "    prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=DEFAULT_TEMPLATE)\n",
    "    retriever = vector_db.as_retriever(search_type='similarity', search_kwargs={'k': top_k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=retriever,\n",
    "                                        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "                                        return_source_documents=True)\n",
    "    answer = chain({\"query\": prompt, \"top_k\": top_k})\n",
    "    print(f\"answers: {answer}\")\n",
    "    # answer = chain.run(prompt)\n",
    "    # answer = answer['choices'][0]['message']['content']\n",
    "    answer = answer['result']\n",
    "    return answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
