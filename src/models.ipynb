{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    current_dir = os.getcwd()\n",
    "except NameError:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set path，temporary path expansion\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import time\n",
    "from typing import Dict, Any, Mapping, Optional, List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "from vllm import LLM as VLLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from tool import skip_execution\n",
    "from Embedding import (\n",
    "    load_embeddings_faiss,\n",
    "    load_embeddings_chroma,\n",
    "    load_embeddings_milvus,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840eb4d",
   "metadata": {},
   "source": [
    "### 本地部署模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "IS_SKIP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"输入: {prompt}\")\n",
    "        print(f\"输出: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def download_model(localpath, modelname):\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=modelname,\n",
    "        local_dir=localpath,\n",
    "        local_dir_use_symlinks=False,  # Windows 必加\n",
    "        allow_patterns=[\"*.json\", \"*.bin\", \"*.txt\", \"*.model\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_llm_model(\n",
    "    prompt: str = None,\n",
    "    model_name: str = None,\n",
    "    temperature: float = 0.0,\n",
    "    max_token: int = 2048,\n",
    "    n_ctx: int = 512,\n",
    "):\n",
    "    \"\"\"\n",
    "    根据模型名称去加载模型，返回response数据\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(project_dir, \"model\", model_name)\n",
    "    print(model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        download_model(model_path, model_name)\n",
    "\n",
    "    # 配置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature, max_tokens=max_token, top_p=0.95\n",
    "    )\n",
    "\n",
    "    # 初始化VLLM\n",
    "    llm = VLLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=1,  # 根据GPU数量调整\n",
    "        gpu_memory_utilization=0.8,\n",
    "        max_num_batched_tokens=n_ctx,\n",
    "        max_model_len=n_ctx,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    # 生成响应\n",
    "    response = llm.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个智能超级助手，请用专业的词语回答问题，整体上下文带有逻辑性，如果不知道，请不要乱说\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    cost = time.time() - start\n",
    "    print(f\"模型生成时间：{cost}\")\n",
    "    print(f\"大模型回复：\\n{response}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66318b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_get_llm():\n",
    "    outputs = get_llm_model(\"你是谁\", MODEL_NAME, 0.8, 1024, 512)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "\n",
    "test_get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f201a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenLLM(LLM):\n",
    "    \"\"\"\n",
    "    基于VLLM的自定义QwenLLM\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"\"\n",
    "    # 访问时延上限\n",
    "    request_timeout: float = None\n",
    "    # 温度系数\n",
    "    temperature: float = 0.8\n",
    "    # 窗口大小\n",
    "    n_ctx: int = 2048\n",
    "    # token大小\n",
    "    max_tokens: int = 1024\n",
    "    # 并行计算数量\n",
    "    tensor_parallel_size: int = 1\n",
    "    # 模型参数\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    # VLLM实例\n",
    "    _llm: Optional[VLLM] = None\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        self._initialize_llm()\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"初始化VLLM实例\"\"\"\n",
    "        model_path = os.path.join(project_dir, \"model\", self.model_name)\n",
    "        print(\"qwen_path:\", model_path)\n",
    "\n",
    "        self._llm = VLLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=self.tensor_parallel_size,\n",
    "            gpu_memory_utilization=0.8,\n",
    "            max_num_batched_tokens=self.n_ctx,\n",
    "            max_model_len=self.n_ctx,\n",
    "            **self.model_kwargs,\n",
    "        )\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        # 配置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            top_p=0.95,\n",
    "            stop=stop or [],\n",
    "        )\n",
    "\n",
    "        # 生成响应\n",
    "        response = self._llm.chat(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "\n",
    "        print(f\"Qwen response: \\n{response}\")\n",
    "        return response[0].outputs[0].text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vllm-qwen\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"request_timeout\": self.request_timeout,\n",
    "            \"n_ctx\": self.n_ctx,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"tensor_parallel_size\": self.tensor_parallel_size,\n",
    "        }\n",
    "        return {**normal_params, **self.model_kwargs}\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model_name\": self.model_name}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_with_langchain():\n",
    "    # 初始化模型\n",
    "    llm = QwenLLM(model_name=MODEL_NAME, temperature=0.5)\n",
    "\n",
    "    # 创建一个简单的链\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"], template=\"请回答以下问题: {question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # 运行测试\n",
    "    result = chain.run(\"什么是机器学习？\")\n",
    "    print(f\"LangChain测试结果: {result}\")\n",
    "\n",
    "\n",
    "test_with_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8314d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从标志结束的位置开始截取 清楚前缀文字\n",
    "def extract_after_flag(text, flag):\n",
    "    # 找到标志的起始索引\n",
    "    index = text.find(flag)\n",
    "    if index != -1:\n",
    "        return text[index + len(flag) :]\n",
    "    return \"\"  # 如果没有找到标志，返回空字符串\n",
    "\n",
    "\n",
    "def ask_and_get_answer_from_local(model_name, vector_db, prompt, template, top_k=5):\n",
    "    \"\"\"\n",
    "    从本地加载大模型\n",
    "    \"\"\"\n",
    "    llm = QwenLLM(model_name=model_name, temperature=0.4)\n",
    "    # region 测试用\n",
    "    if not IS_SKIP:  #  创建基础提示模板（无上下文） 直接生成回答 测试的时候用来对比输出\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=template.replace(\"{context}\\n\", \"\"),  # 移除上下文占位符\n",
    "        )\n",
    "\n",
    "        prompt_text = prompt_template.format(question=prompt)\n",
    "        direct_answer = llm(prompt_text)\n",
    "        print(f\"direct answers: {direct_answer}\")\n",
    "    # endregion\n",
    "\n",
    "    # RAG\n",
    "    docs_and_scores = vector_db.similarity_search_with_score(prompt, k=top_k)\n",
    "    print(\"docs_and_scores: \", docs_and_scores)\n",
    "    # knowledge = [doc[\"page_content\"] for doc in docs_and_scores] # 根据不同模型要调整字段\n",
    "    # print(\"检索到的知识：\", knowledge)\n",
    "\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": top_k}\n",
    "    )\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"], template=template\n",
    "    )\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    answer = chain({\"query\": prompt, \"top_k\": top_k})\n",
    "    # answer = chain.run(prompt)\n",
    "    # answer = answer['choices'][0]['message']['content']\n",
    "    answer = answer[\"result\"]\n",
    "    print(f\"answers: {answer}\")\n",
    "    answer = extract_after_flag(answer, \"</think>\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"\n",
    "        你是一个聪明的超级智能助手，请用专业且富有逻辑顺序的句子回复，并以中文形式且markdown形式输出。\n",
    "        检索到的信息：\n",
    "        {context}\n",
    "        问题：\n",
    "        {question}\n",
    "    \"\"\"\n",
    "prompt = \"'少小离家老大回'的下一句诗是什么\"\n",
    "\n",
    "embedding_name = \"BAAI/bge-small-zh\"\n",
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_faiss():\n",
    "    faiss_db = load_embeddings_faiss(\n",
    "        embedding_name, vector_db_path=os.path.join(project_dir, \"db/faiss_db\")\n",
    "    )\n",
    "    ask_and_get_answer_from_local(MODEL_NAME, faiss_db, prompt, DEFAULT_TEMPLATE)\n",
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_chroma():\n",
    "    chroma_db = load_embeddings_chroma(\n",
    "        embedding_name, persist_dir=os.path.join(project_dir, \"db/chroma_db\")\n",
    "    )\n",
    "    ask_and_get_answer_from_local(MODEL_NAME, chroma_db, prompt, DEFAULT_TEMPLATE)\n",
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_milvus():\n",
    "    milvus_db = load_embeddings_milvus(embedding_name)\n",
    "    ask_and_get_answer_from_local(MODEL_NAME, milvus_db, prompt, DEFAULT_TEMPLATE)\n",
    "\n",
    "\n",
    "test_getRagAnswer_chroma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988def12",
   "metadata": {},
   "source": [
    "### 在线模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91521051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from openai import OpenAI\n",
    "from pydantic import Field\n",
    "\n",
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "ONLINE_MODEL_NAME = \"qwen-plus\"\n",
    "\n",
    "\n",
    "class OpenAILLM(LLM):\n",
    "    \"\"\"\n",
    "    一个将OpenAI API包装为LangChain LLM的自定义类\n",
    "    \"\"\"\n",
    "\n",
    "    api_key: Optional[str] = Field(None)\n",
    "    base_url: Optional[str] = Field(BASE_URL)\n",
    "    model_name: str = Field(ONLINE_MODEL_NAME)\n",
    "    temperature: float = Field(0.7)\n",
    "    max_tokens: int = Field(1024)\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        # 配置OpenAI API密钥\n",
    "        if self.api_key:\n",
    "            OpenAI.api_key = self.api_key\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"返回LLM类型标识\"\"\"\n",
    "        return self.model_name\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"返回用于标识LLM的参数\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "        }\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        实现调用OpenAI API的核心方法\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 调用OpenAI的 completions API\n",
    "            client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=self.base_url,\n",
    "            )\n",
    "            completion = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "            )\n",
    "            # 提取并返回生成的文本\n",
    "            return completion.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"调用OpenAI API时发生错误: {str(e)}\")\n",
    "\n",
    "    def predict(self, text: str, **kwargs: Any) -> str:\n",
    "        \"\"\"预测方法，与LangChain其他组件兼容\"\"\"\n",
    "        return self._call(text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(\n",
    "    model_name, url, vector_db, prompt, template, top_k=3, api_key=None\n",
    "):\n",
    "    api_key = api_key or os.getenv(\"QWEN_API_KEY\") or os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "    llm = OpenAILLM(model_name=model_name, base_url=url, api_key=api_key)\n",
    "    # region 测试用\n",
    "    if not IS_SKIP:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"question\"],\n",
    "            template=template.replace(\"{context}\\n\", \"\"),  # 移除上下文占位符\n",
    "        )\n",
    "\n",
    "        prompt_text = prompt_template.format(question=prompt)\n",
    "        direct_answer = llm(prompt_text)\n",
    "        print(f\"direct answers: {direct_answer}\")\n",
    "    # endregion\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": top_k}\n",
    "    )\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"], template=template\n",
    "    )\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    answer = chain({\"query\": prompt, \"top_k\": top_k})\n",
    "    answer = answer[\"result\"]\n",
    "    print(f\"answers: {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_chroma():\n",
    "    chroma_db = load_embeddings_chroma(\n",
    "        embedding_name, persist_dir=os.path.join(project_dir, \"db/chroma_db\")\n",
    "    )\n",
    "    ask_and_get_answer(ONLINE_MODEL_NAME, BASE_URL, chroma_db, prompt, DEFAULT_TEMPLATE)\n",
    "\n",
    "\n",
    "test_getRagAnswer_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试法律向量库\n",
    "prompt = \"杀人自首以后，怎么判刑\"\n",
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer_law():\n",
    "    chroma_db = load_embeddings_faiss(\n",
    "        embedding_name, persist_dir=os.path.join(project_dir, \"db/law_db\")\n",
    "    )\n",
    "    ask_and_get_answer_from_local(MODEL_NAME, chroma_db, prompt, DEFAULT_TEMPLATE)\n",
    "\n",
    "\n",
    "test_getRagAnswer_law()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
