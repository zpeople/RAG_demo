{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3adcc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Any, Mapping, Optional, List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "from vllm import LLM as VLLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from tool import skip_execution\n",
    "from Embedding import load_embeddings_faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "301f92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME =\"Qwen/Qwen3-0.6B\"\n",
    "IS_SKIP=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e49d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"输入: {prompt}\")\n",
    "        print(f\"输出: {generated_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "def download_model(localpath,modelname):\n",
    "    from huggingface_hub import snapshot_download\n",
    "    snapshot_download(\n",
    "        repo_id=modelname, \n",
    "        local_dir=localpath,\n",
    "        local_dir_use_symlinks=False,  # Windows 必加\n",
    "        allow_patterns=[\"*.json\", \"*.bin\", \"*.txt\", \"*.model\"] \n",
    "    )\n",
    "    \n",
    "def get_llm_model(\n",
    "        prompt: str = None,\n",
    "        model: str = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_token: int = 2048,\n",
    "        n_ctx: int = 512):\n",
    "    \"\"\"\n",
    "    根据模型名称去加载模型，返回response数据\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(\"../model\",model)\n",
    "    print(model_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        download_model(model_path,model)\n",
    "\n",
    "    # 配置采样参数\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_token,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # 初始化VLLM\n",
    "    llm = VLLM(\n",
    "        model=model_path,\n",
    "        tensor_parallel_size=1,  # 根据GPU数量调整\n",
    "        gpu_memory_utilization=0.9,\n",
    "        max_num_batched_tokens=n_ctx,\n",
    "        max_model_len=10000,\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    # 生成响应\n",
    "    response = llm.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"你是一个智能超级助手，请用专业的词语回答问题，整体上下文带有逻辑性，如果不知道，请不要乱说\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    cost = time.time() - start\n",
    "    print(f\"模型生成时间：{cost}\")\n",
    "    print(f\"大模型回复：\\n{response}\")\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66318b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_get_llm():\n",
    "    outputs =get_llm_model(\"你是谁\",MODEL_NAME,0.8,1024,512)\n",
    "    print_outputs(outputs)\n",
    "\n",
    "# test_get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f201a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenLLM(LLM):\n",
    "    \"\"\"\n",
    "    基于VLLM的自定义QwenLLM\n",
    "    \"\"\"\n",
    "    model_name: str = \"\"\n",
    "    # 访问时延上限\n",
    "    request_timeout: float = None\n",
    "    # 温度系数\n",
    "    temperature: float = 0.8\n",
    "    # 窗口大小\n",
    "    n_ctx :int =2048\n",
    "    # token大小\n",
    "    max_tokens:int= 1024\n",
    "    # 并行计算数量\n",
    "    tensor_parallel_size: int = 1\n",
    "    # 模型参数\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    # VLLM实例\n",
    "    _llm: Optional[VLLM] = None\n",
    "\n",
    "    def __init__(self, **data: Any):\n",
    "        super().__init__(** data)\n",
    "        self._initialize_llm()\n",
    "\n",
    "    def _initialize_llm(self):\n",
    "        \"\"\"初始化VLLM实例\"\"\"\n",
    "        model_path = os.path.join(\"../model\",self.model_name)\n",
    "        print(\"qwen_path:\", model_path)\n",
    "        \n",
    "        self._llm = VLLM(\n",
    "            model=model_path,\n",
    "            tensor_parallel_size=self.tensor_parallel_size,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_num_batched_tokens=self.n_ctx,\n",
    "            max_model_len=10000,\n",
    "            **self.model_kwargs\n",
    "        )\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              ** kwargs: Any):\n",
    "        # 配置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            top_p=0.95,\n",
    "            stop=stop or []\n",
    "        )\n",
    "        \n",
    "        # 生成响应\n",
    "        response = self._llm.chat(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"你是一个智能超级助手，请用[中文]专业的词语回答问题，整体上下文带有逻辑性，并以markdown格式输出\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "            ],\n",
    "            sampling_params=sampling_params\n",
    "        )\n",
    "\n",
    "        print(f\"Qwen response: \\n{response}\")\n",
    "        return response[0].outputs[0].text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"vllm-qwen\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取调用默认参数。\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"request_timeout\": self.request_timeout,\n",
    "            \"n_ctx\": self.n_ctx,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"tensor_parallel_size\": self.tensor_parallel_size\n",
    "        }\n",
    "        return {**normal_params, **self.model_kwargs}\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model_name\": self.model_name}, **self._default_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e75a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@skip_execution(IS_SKIP)\n",
    "def test_with_langchain():\n",
    "    # 初始化模型\n",
    "    llm = QwenLLM(\n",
    "        model_name=MODEL_NAME,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    # 创建一个简单的链\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"请回答以下问题: {question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # 运行测试\n",
    "    result = chain.run(\"什么是机器学习？\")\n",
    "    print(f\"LangChain测试结果: {result}\")\n",
    "\n",
    "\n",
    "# test_with_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8314d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ask_and_get_answer_from_local(model_name, vector_db, prompt,template, top_k=5):\n",
    "    \"\"\"\n",
    "    从本地加载大模型\n",
    "    :param model_name: 模型名称\n",
    "    :param vector_db:\n",
    "    :param prompt:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    llm = QwenLLM(model_name=model_name, temperature=0.4)\n",
    "    # 创建基础提示模板（无上下文） 直接生成回答\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=template.replace(\"{context}\\n\", \"\")  # 移除上下文占位符\n",
    "    )\n",
    "    \n",
    "    prompt_text = prompt_template.format(question=prompt)\n",
    "    dir_answer = llm(prompt_text)  # 假设QwenLLM实例可直接调用生成回答\n",
    "    print(f\"direct answers: {dir_answer}\")\n",
    "\n",
    "    # RAG \n",
    "    docs_and_scores = vector_db.similarity_search_with_score(prompt, k=top_k)\n",
    "    print(\"docs_and_scores: \", docs_and_scores)\n",
    "    # knowledge = [doc.page_content for doc in docs_and_scores]\n",
    "    # print(\"检索到的知识：\", knowledge)\n",
    "\n",
    "    prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    retriever = vector_db.as_retriever(search_type='similarity', search_kwargs={'k': top_k})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=retriever,\n",
    "                                        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "                                        return_source_documents=True)\n",
    "    answer = chain({\"query\": prompt, \"top_k\": top_k})\n",
    "    print(f\"answers: {answer}\")\n",
    "    # answer = chain.run(prompt)\n",
    "    # answer = answer['choices'][0]['message']['content']\n",
    "    answer = answer['result']\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-02 18:12:59] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: ../model/bge-Small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/bge-Small\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open ../db/chroma_db/index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2791/2670118547.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0mfaiss_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings_faiss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bge-Small\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_db_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../db/chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mask_and_get_answer_from_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaiss_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'幽意无断绝'的下一句是什么\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEFAULT_TEMPLATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_getRagAnswer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RAG_demo/src/tool.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0;31m# 如果skip为True，则不执行函数，返回None或提示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"函数 {func.__name__} 已跳过执行\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2791/2670118547.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0m问题\u001b[0m\u001b[0;31m：\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfaiss_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings_faiss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bge-Small\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_db_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../db/chroma_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mask_and_get_answer_from_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaiss_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'幽意无断绝'的下一句是什么\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEFAULT_TEMPLATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAG_demo/src/Embedding.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(embedding_name, vector_db_path)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \"\"\"\n\u001b[1;32m    154\u001b[0m     \u001b[0m加载向量库\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[1;32m    156\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_db_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dangerous_deserialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rag_py/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             )\n\u001b[1;32m   1202\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rag_py/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   9848\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9849\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open ../db/chroma_db/index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "@skip_execution(IS_SKIP)\n",
    "def test_getRagAnswer():\n",
    "    DEFAULT_TEMPLATE = \"\"\"\n",
    "        你是一个聪明的超级智能助手，请用专业且富有逻辑顺序的句子回复，并以中文形式且markdown形式输出。\n",
    "        检索到的信息：\n",
    "        {context}\n",
    "        问题：\n",
    "        {question}\n",
    "    \"\"\"\n",
    "    faiss_db = load_embeddings_faiss(embedding_name=\"bge-Small\",vector_db_path=\"../db/faiss_db\")\n",
    "    ask_and_get_answer_from_local(MODEL_NAME,faiss_db,\"'幽意无断绝'的下一句是什么\",DEFAULT_TEMPLATE)\n",
    "\n",
    "test_getRagAnswer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d332c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
